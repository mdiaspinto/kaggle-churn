{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b5cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import src.preprocessing\n",
    "from importlib import reload\n",
    "reload(src.preprocessing)\n",
    "\n",
    "from typing import Union\n",
    "from src.preprocessing import compute_cancellation, aggregate_user_day_activity, add_days_since, add_rolling_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca68f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>gender</th>\n",
       "      <th>firstName</th>\n",
       "      <th>level</th>\n",
       "      <th>lastName</th>\n",
       "      <th>userId</th>\n",
       "      <th>ts</th>\n",
       "      <th>auth</th>\n",
       "      <th>page</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>location</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>method</th>\n",
       "      <th>length</th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>time</th>\n",
       "      <th>registration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538352001000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>278</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>524.32934</td>\n",
       "      <td>Ich mache einen Spiegel - Dream Part 4</td>\n",
       "      <td>Popol Vuh</td>\n",
       "      <td>2018-10-01 00:00:01</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538352525000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>279</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>178.02404</td>\n",
       "      <td>Monster (Album Version)</td>\n",
       "      <td>Skillet</td>\n",
       "      <td>2018-10-01 00:08:45</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538352703000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>280</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>232.61995</td>\n",
       "      <td>Seven Nation Army</td>\n",
       "      <td>The White Stripes</td>\n",
       "      <td>2018-10-01 00:11:43</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538352935000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>281</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>265.50812</td>\n",
       "      <td>Under The Bridge (Album Version)</td>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>2018-10-01 00:15:35</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538353200000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>282</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>471.69261</td>\n",
       "      <td>Circlesong 6</td>\n",
       "      <td>Bobby McFerrin</td>\n",
       "      <td>2018-10-01 00:20:00</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      status gender firstName level lastName   userId             ts  \\\n",
       "0        200      M     Shlok  paid  Johnson  1749042  1538352001000   \n",
       "992      200      M     Shlok  paid  Johnson  1749042  1538352525000   \n",
       "1360     200      M     Shlok  paid  Johnson  1749042  1538352703000   \n",
       "1825     200      M     Shlok  paid  Johnson  1749042  1538352935000   \n",
       "2366     200      M     Shlok  paid  Johnson  1749042  1538353200000   \n",
       "\n",
       "           auth      page  sessionId                         location  \\\n",
       "0     Logged In  NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "992   Logged In  NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "1360  Logged In  NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "1825  Logged In  NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "2366  Logged In  NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "\n",
       "      itemInSession                                          userAgent method  \\\n",
       "0               278  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "992             279  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "1360            280  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "1825            281  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "2366            282  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "\n",
       "         length                                    song  \\\n",
       "0     524.32934  Ich mache einen Spiegel - Dream Part 4   \n",
       "992   178.02404                 Monster (Album Version)   \n",
       "1360  232.61995                       Seven Nation Army   \n",
       "1825  265.50812        Under The Bridge (Album Version)   \n",
       "2366  471.69261                            Circlesong 6   \n",
       "\n",
       "                     artist                time        registration  \n",
       "0                 Popol Vuh 2018-10-01 00:00:01 2018-08-08 13:22:21  \n",
       "992                 Skillet 2018-10-01 00:08:45 2018-08-08 13:22:21  \n",
       "1360      The White Stripes 2018-10-01 00:11:43 2018-08-08 13:22:21  \n",
       "1825  Red Hot Chili Peppers 2018-10-01 00:15:35 2018-08-08 13:22:21  \n",
       "2366         Bobby McFerrin 2018-10-01 00:20:00 2018-08-08 13:22:21  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = '/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn'\n",
    "df = pd.read_parquet(root + '/data/train.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58236d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = aggregate_user_day_activity(df)\n",
    "df_new  = add_days_since(df_new)\n",
    "df_new = add_rolling_averages(df_new, columns=['Add Friend', 'Add to Playlist', 'Thumbs Down', 'Thumbs Up'], n=30)\n",
    "df_new = add_rolling_averages(df_new, columns=['Thumbs Down', 'Thumbs Up'], n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1131c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cancellation targets for 51 unique dates...\n",
      "\n",
      "Cancellation targets shape: (976140, 3)\n",
      "Sample:\n",
      "    userId  will_churn        date\n",
      "0  1749042           0  2018-10-01\n",
      "1  1563081           0  2018-10-01\n",
      "2  1697168           0  2018-10-01\n",
      "3  1222580           0  2018-10-01\n",
      "4  1714398           0  2018-10-01\n",
      "5  1010522           0  2018-10-01\n",
      "6  1475659           0  2018-10-01\n",
      "7  1558463           0  2018-10-01\n",
      "8  1605667           0  2018-10-01\n",
      "9  1385500           0  2018-10-01\n",
      "\n",
      "Cancellation targets shape: (976140, 3)\n",
      "Sample:\n",
      "    userId  will_churn        date\n",
      "0  1749042           0  2018-10-01\n",
      "1  1563081           0  2018-10-01\n",
      "2  1697168           0  2018-10-01\n",
      "3  1222580           0  2018-10-01\n",
      "4  1714398           0  2018-10-01\n",
      "5  1010522           0  2018-10-01\n",
      "6  1475659           0  2018-10-01\n",
      "7  1558463           0  2018-10-01\n",
      "8  1605667           0  2018-10-01\n",
      "9  1385500           0  2018-10-01\n"
     ]
    }
   ],
   "source": [
    "# Compute cancellation target for each date in df_new\n",
    "# For each unique date, compute which users cancelled within 10 days\n",
    "\n",
    "unique_dates = sorted(df_new['date'].unique())\n",
    "print(f\"Computing cancellation targets for {len(unique_dates)} unique dates...\")\n",
    "\n",
    "cancellation_targets = []\n",
    "\n",
    "for present_date in unique_dates:\n",
    "    # Compute cancellation for this date\n",
    "    target_df = compute_cancellation(df, present_time=present_date, window_days=10)\n",
    "    # Add the date column\n",
    "    target_df['date'] = present_date\n",
    "    cancellation_targets.append(target_df)\n",
    "\n",
    "# Combine all targets\n",
    "target_by_date = pd.concat(cancellation_targets, ignore_index=True)\n",
    "\n",
    "# Rename columns for clarity\n",
    "target_by_date = target_by_date.rename(columns={'userId': 'userId', 'will_churn': 'churn_status'})\n",
    "\n",
    "print(f\"\\nCancellation targets shape: {target_by_date.shape}\")\n",
    "print(f\"Sample:\")\n",
    "print(target_by_date.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c09f0107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved df_new to data/df_transformed.csv ((622838, 29))\n",
      "Saved target_by_date to data/churn_status.csv ((976140, 3))\n"
     ]
    }
   ],
   "source": [
    "# Save df_new and target_by_date as CSV files\n",
    "df_new.to_csv(root + '/data/df_transformed.csv', index=False)\n",
    "target_by_date.to_csv(root + '/data/churn_status.csv', index=False)\n",
    "\n",
    "print(f\"Saved df_new to data/df_transformed.csv ({df_new.shape})\")\n",
    "print(f\"Saved target_by_date to data/churn_status.csv ({target_by_date.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1cee51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape: (622838, 30)\n",
      "Columns: ['date', 'userId', 'About', 'Add Friend', 'Add to Playlist', 'Cancel', 'Downgrade', 'Error', 'Help', 'Home', 'Logout', 'NextSong', 'Roll Advert', 'Save Settings', 'Settings', 'Submit Downgrade', 'Submit Upgrade', 'Thumbs Down', 'Thumbs Up', 'Upgrade', 'days_since_submit_downgrade', 'days_since_submit_upgrade', 'days_since_cancel', 'add_friend_avg_30d', 'add_to_playlist_avg_30d', 'thumbs_down_avg_30d', 'thumbs_up_avg_30d', 'thumbs_down_avg_7d', 'thumbs_up_avg_7d', 'churn_status']\n",
      "\n",
      "Churn distribution:\n",
      "churn_status\n",
      "0    587760\n",
      "1     35078\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample:\n",
      "         date   userId  About  Add Friend  Add to Playlist  Cancel  Downgrade  \\\n",
      "0  2018-10-02  1000025      0           3                2       0          1   \n",
      "1  2018-10-03  1000025      0           6               13       0          2   \n",
      "2  2018-10-04  1000025      0           0                5       0          3   \n",
      "3  2018-10-05  1000025      0           0                1       0          2   \n",
      "4  2018-10-06  1000025      0           0                0       0          0   \n",
      "\n",
      "   Error  Help  Home  ...  days_since_submit_downgrade  \\\n",
      "0      0     1     6  ...                         None   \n",
      "1      0     2    13  ...                         None   \n",
      "2      0     1     9  ...                         None   \n",
      "3      0     0     2  ...                         None   \n",
      "4      0     0     0  ...                         None   \n",
      "\n",
      "   days_since_submit_upgrade  days_since_cancel  add_friend_avg_30d  \\\n",
      "0                          0               None                 NaN   \n",
      "1                          1               None                3.00   \n",
      "2                          2               None                4.50   \n",
      "3                          3               None                3.00   \n",
      "4                          4               None                2.25   \n",
      "\n",
      "   add_to_playlist_avg_30d  thumbs_down_avg_30d  thumbs_up_avg_30d  \\\n",
      "0                      NaN                  NaN                NaN   \n",
      "1                 2.000000             2.000000           5.000000   \n",
      "2                 7.500000             2.500000          13.000000   \n",
      "3                 6.666667             2.666667          12.666667   \n",
      "4                 5.250000             2.000000           9.500000   \n",
      "\n",
      "   thumbs_down_avg_7d  thumbs_up_avg_7d  churn_status  \n",
      "0                 NaN               NaN             0  \n",
      "1            2.000000          5.000000             0  \n",
      "2            2.500000         13.000000             0  \n",
      "3            2.666667         12.666667             0  \n",
      "4            2.000000          9.500000             0  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge df_new with target_by_date to create training dataset\n",
    "df_train = df_new.merge(target_by_date, on=['userId', 'date'], how='left')\n",
    "\n",
    "print(f\"df_train shape: {df_train.shape}\")\n",
    "print(f\"Columns: {df_train.columns.tolist()}\")\n",
    "print(f\"\\nChurn distribution:\")\n",
    "print(df_train['churn_status'].value_counts())\n",
    "print(f\"\\nSample:\")\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50573aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape after dropping NaN targets: (622838, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/x1vfxvpd1_q07my0mb9rc8tm0000gn/T/ipykernel_3092/629828143.py:16: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X = df_model[feature_cols].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features: 27 columns\n",
      "Feature columns: ['About', 'Add Friend', 'Add to Playlist', 'Cancel', 'Downgrade', 'Error', 'Help', 'Home', 'Logout', 'NextSong', 'Roll Advert', 'Save Settings', 'Settings', 'Submit Downgrade', 'Submit Upgrade', 'Thumbs Down', 'Thumbs Up', 'Upgrade', 'days_since_submit_downgrade', 'days_since_submit_upgrade', 'days_since_cancel', 'add_friend_avg_30d', 'add_to_playlist_avg_30d', 'thumbs_down_avg_30d', 'thumbs_up_avg_30d', 'thumbs_down_avg_7d', 'thumbs_up_avg_7d']\n",
      "\n",
      "Target distribution:\n",
      "churn_status\n",
      "0    587760\n",
      "1     35078\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train set: (498270, 27), Test set: (124568, 27)\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Train set: (498270, 27), Test set: (124568, 27)\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "=== Model Performance ===\n",
      "ROC-AUC Score: 0.7257\n",
      "\n",
      "Confusion Matrix:\n",
      "[[117552      0]\n",
      " [  6150    866]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97    117552\n",
      "           1       1.00      0.12      0.22      7016\n",
      "\n",
      "    accuracy                           0.95    124568\n",
      "   macro avg       0.98      0.56      0.60    124568\n",
      "weighted avg       0.95      0.95      0.93    124568\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                      feature  importance\n",
      "3                      Cancel    0.687899\n",
      "23        thumbs_down_avg_30d    0.066453\n",
      "22    add_to_playlist_avg_30d    0.028525\n",
      "9                    NextSong    0.023544\n",
      "25         thumbs_down_avg_7d    0.022910\n",
      "21         add_friend_avg_30d    0.021418\n",
      "24          thumbs_up_avg_30d    0.019896\n",
      "4                   Downgrade    0.019609\n",
      "10                Roll Advert    0.017128\n",
      "19  days_since_submit_upgrade    0.013218\n",
      "\n",
      "=== Model Performance ===\n",
      "ROC-AUC Score: 0.7257\n",
      "\n",
      "Confusion Matrix:\n",
      "[[117552      0]\n",
      " [  6150    866]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97    117552\n",
      "           1       1.00      0.12      0.22      7016\n",
      "\n",
      "    accuracy                           0.95    124568\n",
      "   macro avg       0.98      0.56      0.60    124568\n",
      "weighted avg       0.95      0.95      0.93    124568\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                      feature  importance\n",
      "3                      Cancel    0.687899\n",
      "23        thumbs_down_avg_30d    0.066453\n",
      "22    add_to_playlist_avg_30d    0.028525\n",
      "9                    NextSong    0.023544\n",
      "25         thumbs_down_avg_7d    0.022910\n",
      "21         add_friend_avg_30d    0.021418\n",
      "24          thumbs_up_avg_30d    0.019896\n",
      "4                   Downgrade    0.019609\n",
      "10                Roll Advert    0.017128\n",
      "19  days_since_submit_upgrade    0.013218\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest model to predict churn_status\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Prepare data: drop rows with NaN in target\n",
    "df_model = df_train.dropna(subset=['churn_status']).copy()\n",
    "print(f\"Training data shape after dropping NaN targets: {df_model.shape}\")\n",
    "\n",
    "# Separate features and target\n",
    "# Exclude userId, date, and churn_status from features\n",
    "exclude_cols = ['userId', 'date', 'churn_status']\n",
    "feature_cols = [col for col in df_model.columns if col not in exclude_cols]\n",
    "\n",
    "# Handle remaining NaN values in features (fill with 0 or median)\n",
    "X = df_model[feature_cols].fillna(0)\n",
    "y = df_model['churn_status'].astype(int)\n",
    "\n",
    "print(f\"\\nFeatures: {len(feature_cols)} columns\")\n",
    "print(f\"Feature columns: {feature_cols}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n=== Model Performance ===\")\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dcdeb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Test data shape: (4393179, 19)\n",
      "Date range: 2018-10-01 00:00:06 to 2018-11-20 00:00:00\n",
      "\n",
      "Applying transformations to test data...\n",
      "Test data shape: (4393179, 19)\n",
      "Date range: 2018-10-01 00:00:06 to 2018-11-20 00:00:00\n",
      "\n",
      "Applying transformations to test data...\n",
      "After aggregation: (117479, 22)\n",
      "Available columns: ['date', 'userId', 'About', 'Add Friend', 'Add to Playlist', 'Downgrade', 'Error', 'Help', 'Home', 'Login', 'Logout', 'NextSong', 'Register', 'Roll Advert', 'Save Settings', 'Settings', 'Submit Downgrade', 'Submit Registration', 'Submit Upgrade', 'Thumbs Down', 'Thumbs Up', 'Upgrade']\n",
      "After aggregation: (117479, 22)\n",
      "Available columns: ['date', 'userId', 'About', 'Add Friend', 'Add to Playlist', 'Downgrade', 'Error', 'Help', 'Home', 'Login', 'Logout', 'NextSong', 'Register', 'Roll Advert', 'Save Settings', 'Settings', 'Submit Downgrade', 'Submit Registration', 'Submit Upgrade', 'Thumbs Down', 'Thumbs Up', 'Upgrade']\n",
      "After days_since: (117479, 24)\n",
      "After days_since: (117479, 24)\n",
      "After 30d rolling averages: (117479, 28)\n",
      "After 7d rolling averages: (117479, 30)\n",
      "\n",
      "Test data ready for prediction!\n",
      "Final test features shape: (117479, 30)\n",
      "Columns: ['date', 'userId', 'About', 'Add Friend', 'Add to Playlist', 'Downgrade', 'Error', 'Help', 'Home', 'Login', 'Logout', 'NextSong', 'Register', 'Roll Advert', 'Save Settings', 'Settings', 'Submit Downgrade', 'Submit Registration', 'Submit Upgrade', 'Thumbs Down', 'Thumbs Up', 'Upgrade', 'days_since_submit_downgrade', 'days_since_submit_upgrade', 'add_friend_avg_30d', 'add_to_playlist_avg_30d', 'thumbs_down_avg_30d', 'thumbs_up_avg_30d', 'thumbs_down_avg_7d', 'thumbs_up_avg_7d']\n"
     ]
    }
   ],
   "source": [
    "# Load and transform test data with the same pipeline as training data\n",
    "print(\"Loading test data...\")\n",
    "df_test = pd.read_parquet(root + '/data/test.parquet')\n",
    "print(f\"Test data shape: {df_test.shape}\")\n",
    "print(f\"Date range: {df_test['time'].min()} to {df_test['time'].max()}\")\n",
    "\n",
    "# Apply the same transformations\n",
    "print(\"\\nApplying transformations to test data...\")\n",
    "\n",
    "# 1. Aggregate by user and day\n",
    "df_test_agg = aggregate_user_day_activity(df_test, fill_missing_days=True)\n",
    "print(f\"After aggregation: {df_test_agg.shape}\")\n",
    "print(f\"Available columns: {df_test_agg.columns.tolist()}\")\n",
    "\n",
    "# 2. Add days since metrics - only track columns that exist in test data\n",
    "available_tracking_cols = []\n",
    "for col in ['Submit Downgrade', 'Submit Upgrade', 'Cancel']:\n",
    "    if col in df_test_agg.columns:\n",
    "        available_tracking_cols.append(col)\n",
    "\n",
    "if available_tracking_cols:\n",
    "    df_test_agg = add_days_since(df_test_agg, columns_to_track=available_tracking_cols)\n",
    "    print(f\"After days_since: {df_test_agg.shape}\")\n",
    "else:\n",
    "    print(\"No tracking columns found, skipping days_since\")\n",
    "\n",
    "# 3. Add 30-day rolling averages\n",
    "activity_cols_30 = ['Add Friend', 'Add to Playlist', 'Thumbs Down', 'Thumbs Up']\n",
    "available_cols_30 = [col for col in activity_cols_30 if col in df_test_agg.columns]\n",
    "if available_cols_30:\n",
    "    df_test_agg = add_rolling_averages(df_test_agg, columns=available_cols_30, n=30)\n",
    "    print(f\"After 30d rolling averages: {df_test_agg.shape}\")\n",
    "\n",
    "# 4. Add 7-day rolling averages\n",
    "activity_cols_7 = ['Thumbs Down', 'Thumbs Up']\n",
    "available_cols_7 = [col for col in activity_cols_7 if col in df_test_agg.columns]\n",
    "if available_cols_7:\n",
    "    df_test_agg = add_rolling_averages(df_test_agg, columns=available_cols_7, n=7)\n",
    "    print(f\"After 7d rolling averages: {df_test_agg.shape}\")\n",
    "\n",
    "print(\"\\nTest data ready for prediction!\")\n",
    "print(f\"Final test features shape: {df_test_agg.shape}\")\n",
    "print(f\"Columns: {df_test_agg.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "148ebd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test data for model prediction...\n",
      "Last date in test data: 2018-11-20\n",
      "Number of users on last date: 2\n",
      "\n",
      "Training features: ['About', 'Add Friend', 'Add to Playlist', 'Cancel', 'Downgrade', 'Error', 'Help', 'Home', 'Logout', 'NextSong', 'Roll Advert', 'Save Settings', 'Settings', 'Submit Downgrade', 'Submit Upgrade', 'Thumbs Down', 'Thumbs Up', 'Upgrade', 'days_since_submit_downgrade', 'days_since_submit_upgrade', 'days_since_cancel', 'add_friend_avg_30d', 'add_to_playlist_avg_30d', 'thumbs_down_avg_30d', 'thumbs_up_avg_30d', 'thumbs_down_avg_7d', 'thumbs_up_avg_7d']\n",
      "\n",
      "Using 27 features for prediction\n",
      "Test data shape for prediction: (2, 27)\n",
      "Making predictions...\n",
      "Predictions shape: (2,)\n",
      "Prediction distribution:\n",
      "0    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Submission dataframe shape: (2, 2)\n",
      "Sample submissions:\n",
      "        id  target\n",
      "0  1270265       0\n",
      "1  1835314       0\n",
      "\n",
      "Saved submission to data/submission.csv\n",
      "Submission target distribution:\n",
      "target\n",
      "0    2\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/x1vfxvpd1_q07my0mb9rc8tm0000gn/T/ipykernel_3092/232987868.py:22: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test_pred = df_test_final[feature_cols].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data using only the last date\n",
    "print(\"Preparing test data for model prediction...\")\n",
    "\n",
    "# Get the last date in test data\n",
    "last_date = df_test_agg['date'].max()\n",
    "print(f\"Last date in test data: {last_date}\")\n",
    "\n",
    "# Filter to only predictions for the last date\n",
    "df_test_final = df_test_agg[df_test_agg['date'] == last_date].copy()\n",
    "print(f\"Number of users on last date: {len(df_test_final)}\")\n",
    "\n",
    "# Get the exact feature columns used in training (from feature_cols variable)\n",
    "print(f\"\\nTraining features: {feature_cols}\")\n",
    "\n",
    "# Add missing columns from training to test data with zeros\n",
    "for col in feature_cols:\n",
    "    if col not in df_test_final.columns:\n",
    "        print(f\"Adding missing column: {col}\")\n",
    "        df_test_final[col] = 0\n",
    "\n",
    "# Select features in the same order as training\n",
    "X_test_pred = df_test_final[feature_cols].fillna(0)\n",
    "\n",
    "print(f\"\\nUsing {len(feature_cols)} features for prediction\")\n",
    "print(f\"Test data shape for prediction: {X_test_pred.shape}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "test_predictions = rf_model.predict(X_test_pred)\n",
    "test_predictions_proba = rf_model.predict_proba(X_test_pred)[:, 1]\n",
    "\n",
    "print(f\"Predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Prediction distribution:\")\n",
    "print(pd.Series(test_predictions).value_counts())\n",
    "\n",
    "# Create submission dataframe with userId as id and predictions as target\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test_final['userId'].values,\n",
    "    'target': test_predictions\n",
    "})\n",
    "\n",
    "print(f\"\\nSubmission dataframe shape: {submission.shape}\")\n",
    "print(f\"Sample submissions:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv(root + '/data/submission.csv', index=False)\n",
    "print(f\"\\nSaved submission to data/submission.csv\")\n",
    "print(f\"Submission target distribution:\")\n",
    "print(submission['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many users have observations on the last date in the raw test data\n",
    "last_date_pd = pd.to_datetime(last_date)\n",
    "\n",
    "# Convert time column to datetime for comparison\n",
    "df_test['time_date'] = pd.to_datetime(df_test['time']).dt.date\n",
    "\n",
    "# Count users on the last date\n",
    "users_on_last_date = df_test[df_test['time_date'] == last_date]['userId'].nunique()\n",
    "total_events_on_last_date = len(df_test[df_test['time_date'] == last_date])\n",
    "\n",
    "print(f\"Last date: {last_date}\")\n",
    "print(f\"Unique users with events on {last_date}: {users_on_last_date}\")\n",
    "print(f\"Total events on {last_date}: {total_events_on_last_date}\")\n",
    "\n",
    "# Also check the date range in raw test data\n",
    "print(f\"\\nDate range in raw test data:\")\n",
    "print(f\"First date: {df_test['time_date'].min()}\")\n",
    "print(f\"Last date: {df_test['time_date'].max()}\")\n",
    "print(f\"\\nTotal unique dates in test data: {df_test['time_date'].nunique()}\")\n",
    "print(f\"Total unique users in test data: {df_test['userId'].nunique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
