{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1c283a",
   "metadata": {},
   "source": [
    "## üîí Data Leakage Prevention\n",
    "\n",
    "**Critical Fix:** Test data has NO cancellation events (you're predicting future churn!).\n",
    "\n",
    "**Solution:**\n",
    "1. ‚úÖ **Separate train/predict modes** - Pipeline factory conditionally includes CancellationTargetTransformer\n",
    "2. ‚úÖ **No transformer refitting on test** - Use training-fitted pipeline for predictions\n",
    "3. ‚úÖ **Combined data for cumulative features** - Accumulated features need full user history (train + test)\n",
    "4. ‚úÖ **Proper feature alignment** - Exclude 'Cancel' as a feature (it's the target!)\n",
    "\n",
    "**Impact:** Realistic validation scores and proper Kaggle submission generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1b5cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import src.preprocessing\n",
    "from importlib import reload\n",
    "reload(src.preprocessing)\n",
    "\n",
    "from src.preprocessing import (\n",
    "    aggregate_user_day_activity, \n",
    "    add_rolling_averages,\n",
    "    compute_cancellation_batch\n",
    ")\n",
    "\n",
    "# Import sklearn components\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca68f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING RAW TRAINING DATA\n",
      "================================================================================\n",
      "\n",
      "Raw training data shape: (17499636, 11)\n",
      "Date range: 2018-10-01 00:00:01 to 2018-11-20 00:00:00\n",
      "Unique users: 19140\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD RAW TRAINING DATA\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING RAW TRAINING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "root = '/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn'\n",
    "df_raw = pd.read_parquet(root + '/data/train.parquet')\n",
    "\n",
    "# Clean up: convert object columns to category, drop unnecessary columns\n",
    "object_cols = df_raw.select_dtypes(include=\"object\").columns\n",
    "df_raw[object_cols] = df_raw[object_cols].astype(\"category\")\n",
    "df_raw = df_raw.drop(columns=['gender', 'firstName', 'lastName', 'location', 'userAgent', 'status', 'auth', 'method'])\n",
    "\n",
    "print(f\"\\nRaw training data shape: {df_raw.shape}\")\n",
    "print(f\"Date range: {pd.to_datetime(df_raw['time']).min()} to {pd.to_datetime(df_raw['time']).max()}\")\n",
    "print(f\"Unique users: {df_raw['userId'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3620c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the preprocessing module to pick up changes\n",
    "from importlib import reload\n",
    "import src.preprocessing\n",
    "reload(src.preprocessing)\n",
    "from src.preprocessing import aggregate_user_day_activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de332b",
   "metadata": {},
   "source": [
    "# üîÑ Time-Series Cross-Validation\n",
    "\n",
    "Run proper time-series CV to validate the approach before generating final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d68f349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 3-fold time-series cross-validation...\n",
      "This validates our pipeline with proper temporal splits\n",
      "\n",
      "================================================================================\n",
      "TIME-SERIES CROSS-VALIDATION (3 FOLDS)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn/src/preprocessing.py:1357: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  user_dates = df_raw.groupby(['userId', 'date']).size().reset_index(name='count')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset info:\n",
      "  Total user-days: 976,140\n",
      "  Date range: 2018-10-01 to 2018-11-20\n",
      "  Total events: 17,499,636\n",
      "\n",
      "================================================================================\n",
      "FOLD 1/3\n",
      "================================================================================\n",
      "Train period: 2018-10-01 to 2018-10-13 (244,035 user-days)\n",
      "Val period:   2018-10-13 to 2018-10-26 (244,035 user-days)\n",
      "\n",
      "‚è±Ô∏è  TIMING BREAKDOWN:\n",
      "  1. Filter raw data: 4.1s (9,726,480 events)\n",
      "  2. Create pipeline: 0.0s\n",
      "  3. Fit transformers: 1.8s\n",
      "  4. Transform (detailed breakdown below)...\n",
      "BasicEventAggregator: Aggregating events to user-day level...\n",
      "  Output shape: (497640, 27)\n",
      "  Features: ['About', 'Add Friend', 'Add to Playlist', 'Cancel', 'Cancellation Confirmation', 'Downgrade', 'Error', 'Help', 'Home', 'Logout', 'NextSong', 'Roll Advert', 'Save Settings', 'Settings', 'Submit Downgrade', 'Submit Upgrade', 'Thumbs Down', 'Thumbs Up', 'Upgrade', 'event_count', 'session_count', 'avg_session_length', 'events_per_session', 'level', 'days_since_registration']\n",
      "RollingAverageTransformerModular: Computing 7d rolling averages...\n",
      "RollingAverageTransformerModular: Computing 14d rolling averages...\n",
      "TrendFeaturesTransformer: Computing trend features...\n",
      "AccumulatedFeaturesTransformer: Computing cumulative features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn/src/preprocessing.py:752: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  errors_per_day = df_errors.groupby([self.user_col, 'date']).size().reset_index(name='daily_errors')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added: accumulated_errors, accumulated_unique_artists\n",
      "PageInteractionTransformer: Computing page interaction features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn/src/preprocessing.py:858: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  page_counts = df_pages.groupby([self.user_col, 'date', self.page_col]).size().unstack(fill_value=0).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added: []\n",
      "CancellationTargetTransformerModular: Computing churn targets (window=10d)...\n",
      "  Churn status - 0: 477467, 1: 20173\n",
      "FeaturePreprocessor: Final preprocessing...\n",
      "     TOTAL TRANSFORM TIME: 403.4s ‚ö†Ô∏è\n",
      "Feature matrix - Train: (248820, 62), Val: (248820, 62)\n",
      "Class distribution - Train: 12504/248820 (5.03% churn)\n",
      "                     Val:   7669/248820 (3.08% churn)\n",
      "Training model...\n",
      "  5. Model training: 3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/opt/anaconda3/envs/ds/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6. Evaluation: 0.8s\n",
      "\n",
      "  ‚è±Ô∏è  FOLD TOTAL: 413.2s (6.9 min)\n",
      "\n",
      "Fold 1 Results:\n",
      "  ROC-AUC: 0.6849\n",
      "\n",
      "================================================================================\n",
      "FOLD 2/3\n",
      "================================================================================\n",
      "Train period: 2018-10-01 to 2018-10-26 (488,070 user-days)\n",
      "Val period:   2018-10-26 to 2018-11-08 (244,035 user-days)\n",
      "\n",
      "‚è±Ô∏è  TIMING BREAKDOWN:\n",
      "  1. Filter raw data: 10.4s (14,112,729 events)\n",
      "  2. Create pipeline: 0.1s\n",
      "  3. Fit transformers: 3.9s\n",
      "  4. Transform (detailed breakdown below)...\n",
      "BasicEventAggregator: Aggregating events to user-day level...\n",
      "  Output shape: (746460, 27)\n",
      "  Features: ['About', 'Add Friend', 'Add to Playlist', 'Cancel', 'Cancellation Confirmation', 'Downgrade', 'Error', 'Help', 'Home', 'Logout', 'NextSong', 'Roll Advert', 'Save Settings', 'Settings', 'Submit Downgrade', 'Submit Upgrade', 'Thumbs Down', 'Thumbs Up', 'Upgrade', 'event_count', 'session_count', 'avg_session_length', 'events_per_session', 'level', 'days_since_registration']\n",
      "RollingAverageTransformerModular: Computing 7d rolling averages...\n",
      "RollingAverageTransformerModular: Computing 14d rolling averages...\n",
      "TrendFeaturesTransformer: Computing trend features...\n",
      "AccumulatedFeaturesTransformer: Computing cumulative features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn/src/preprocessing.py:752: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  errors_per_day = df_errors.groupby([self.user_col, 'date']).size().reset_index(name='daily_errors')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added: accumulated_errors, accumulated_unique_artists\n",
      "PageInteractionTransformer: Computing page interaction features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn/src/preprocessing.py:858: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  page_counts = df_pages.groupby([self.user_col, 'date', self.page_col]).size().unstack(fill_value=0).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added: []\n",
      "CancellationTargetTransformerModular: Computing churn targets (window=10d)...\n",
      "  Churn status - 0: 716127, 1: 30333\n",
      "FeaturePreprocessor: Final preprocessing...\n",
      "     TOTAL TRANSFORM TIME: 497.8s ‚ö†Ô∏è\n",
      "Feature matrix - Train: (497640, 62), Val: (248820, 62)\n",
      "Class distribution - Train: 23441/497640 (4.71% churn)\n",
      "                     Val:   6892/248820 (2.77% churn)\n",
      "Training model...\n",
      "  5. Model training: 4.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/opt/anaconda3/envs/ds/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6. Evaluation: 1.0s\n",
      "\n",
      "  ‚è±Ô∏è  FOLD TOTAL: 517.3s (8.6 min)\n",
      "\n",
      "Fold 2 Results:\n",
      "  ROC-AUC: 0.7327\n",
      "\n",
      "================================================================================\n",
      "FOLD 3/3\n",
      "================================================================================\n",
      "Train period: 2018-10-01 to 2018-11-08 (732,105 user-days)\n",
      "Val period:   2018-11-08 to 2018-11-20 (244,035 user-days)\n",
      "\n",
      "‚è±Ô∏è  TIMING BREAKDOWN:\n",
      "  1. Filter raw data: 9.2s (17,499,636 events)\n",
      "  2. Create pipeline: 0.1s\n",
      "  3. Fit transformers: 7.2s\n",
      "  4. Transform (detailed breakdown below)...\n",
      "BasicEventAggregator: Aggregating events to user-day level...\n",
      "  Output shape: (976140, 27)\n",
      "  Features: ['About', 'Add Friend', 'Add to Playlist', 'Cancel', 'Cancellation Confirmation', 'Downgrade', 'Error', 'Help', 'Home', 'Logout', 'NextSong', 'Roll Advert', 'Save Settings', 'Settings', 'Submit Downgrade', 'Submit Upgrade', 'Thumbs Down', 'Thumbs Up', 'Upgrade', 'event_count', 'session_count', 'avg_session_length', 'events_per_session', 'level', 'days_since_registration']\n",
      "RollingAverageTransformerModular: Computing 7d rolling averages...\n",
      "RollingAverageTransformerModular: Computing 14d rolling averages...\n",
      "TrendFeaturesTransformer: Computing trend features...\n",
      "AccumulatedFeaturesTransformer: Computing cumulative features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn/src/preprocessing.py:752: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  errors_per_day = df_errors.groupby([self.user_col, 'date']).size().reset_index(name='daily_errors')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added: accumulated_errors, accumulated_unique_artists\n",
      "PageInteractionTransformer: Computing page interaction features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn/src/preprocessing.py:858: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  page_counts = df_pages.groupby([self.user_col, 'date', self.page_col]).size().unstack(fill_value=0).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added: []\n",
      "CancellationTargetTransformerModular: Computing churn targets (window=10d)...\n",
      "  Churn status - 0: 938477, 1: 37663\n",
      "FeaturePreprocessor: Final preprocessing...\n",
      "     TOTAL TRANSFORM TIME: 562.2s ‚ö†Ô∏è\n",
      "Feature matrix - Train: (746460, 62), Val: (229680, 62)\n",
      "Class distribution - Train: 33299/746460 (4.46% churn)\n",
      "                     Val:   4364/229680 (1.90% churn)\n",
      "Training model...\n",
      "  5. Model training: 5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/opt/anaconda3/envs/ds/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6. Evaluation: 0.8s\n",
      "\n",
      "  ‚è±Ô∏è  FOLD TOTAL: 584.9s (9.7 min)\n",
      "\n",
      "Fold 3 Results:\n",
      "  ROC-AUC: 0.7625\n",
      "\n",
      "================================================================================\n",
      "CROSS-VALIDATION SUMMARY\n",
      "================================================================================\n",
      " fold  train_size  val_size  roc_auc  churn_rate_train  churn_rate_val\n",
      "    1      248820    248820 0.684916          0.050253        0.030821\n",
      "    2      497640    248820 0.732658          0.047104        0.027699\n",
      "    3      746460    229680 0.762506          0.044609        0.019000\n",
      "\n",
      "Mean ROC-AUC: 0.7267 ¬± 0.0391\n",
      "Best fold:    3 (0.7625)\n",
      "Worst fold:   1 (0.6849)\n",
      "\n",
      "================================================================================\n",
      "CROSS-VALIDATION RESULTS\n",
      "================================================================================\n",
      "Mean ROC-AUC: 0.7267 ¬± 0.0391\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TIME-SERIES CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "from importlib import reload\n",
    "import src.preprocessing\n",
    "reload(src.preprocessing)\n",
    "\n",
    "from src.preprocessing import run_time_series_cv, create_feature_pipeline\n",
    "\n",
    "# Run 3-fold time-series cross-validation\n",
    "print(\"Running 3-fold time-series cross-validation...\")\n",
    "print(\"This validates our pipeline with proper temporal splits\\n\")\n",
    "\n",
    "ts_cv_results = run_time_series_cv(df_raw, n_splits=3, window_days=10)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Mean ROC-AUC: {ts_cv_results['mean_roc_auc']:.4f} ¬± {ts_cv_results['std_roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4592d20",
   "metadata": {},
   "source": [
    "# üöÄ Train Final Model & Generate Submission\n",
    "\n",
    "Train on ALL training data and generate predictions for Kaggle test set (leak-free!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c19622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING FINAL MODEL ON ALL TRAINING DATA\n",
      "================================================================================\n",
      "\n",
      "1. Creating TRAINING pipeline...\n",
      "\n",
      "2. Fitting transformers on training data...\n",
      "\n",
      "3. Transforming raw data to features...\n",
      "BasicEventAggregator: Aggregating events to user-day level...\n",
      "  Output shape: (976140, 27)\n",
      "  Features: ['About', 'Add Friend', 'Add to Playlist', 'Cancel', 'Cancellation Confirmation', 'Downgrade', 'Error', 'Help', 'Home', 'Logout', 'NextSong', 'Roll Advert', 'Save Settings', 'Settings', 'Submit Downgrade', 'Submit Upgrade', 'Thumbs Down', 'Thumbs Up', 'Upgrade', 'event_count', 'session_count', 'avg_session_length', 'events_per_session', 'level', 'days_since_registration']\n",
      "RollingAverageTransformerModular: Computing 7d rolling averages...\n",
      "RollingAverageTransformerModular: Computing 14d rolling averages...\n",
      "TrendFeaturesTransformer: Computing trend features...\n",
      "AccumulatedFeaturesTransformer: Computing cumulative features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn/src/preprocessing.py:752: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  errors_per_day = df_errors.groupby([self.user_col, 'date']).size().reset_index(name='daily_errors')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added: accumulated_errors, accumulated_unique_artists\n",
      "PageInteractionTransformer: Computing page interaction features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn/src/preprocessing.py:858: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  page_counts = df_pages.groupby([self.user_col, 'date', self.page_col]).size().unstack(fill_value=0).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added: []\n",
      "CancellationTargetTransformerModular: Computing churn targets (window=10d)...\n",
      "  Churn status - 0: 938477, 1: 37663\n",
      "FeaturePreprocessor: Final preprocessing...\n",
      "   Features shape: (976140, 62)\n",
      "\n",
      "4. Creating temporal train/validation split...\n",
      "   Train: 593,340 samples (2018-10-01 to 2018-10-31)\n",
      "   Val:   382,800 samples (2018-11-01 to 2018-11-20)\n",
      "\n",
      "5. Feature matrix prepared:\n",
      "   Features: 59\n",
      "   Train churn rate: 4.62%\n",
      "   Val churn rate: 2.68%\n",
      "\n",
      "6. Training LightGBM model...\n",
      "   ‚úì Model trained (scale_pos_weight=20.66)\n",
      "\n",
      "7. Evaluating on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/opt/anaconda3/envs/ds/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Optimal threshold: 0.43\n",
      "   Validation balanced accuracy: 0.6875\n",
      "   Validation ROC-AUC: 0.7448\n",
      "\n",
      "   Classification Report (threshold=0.43):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Churn       0.99      0.66      0.79    372536\n",
      "       Churn       0.06      0.71      0.10     10264\n",
      "\n",
      "    accuracy                           0.66    382800\n",
      "   macro avg       0.52      0.69      0.45    382800\n",
      "weighted avg       0.96      0.66      0.78    382800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAIN FINAL MODEL ON ALL TRAINING DATA\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING FINAL MODEL ON ALL TRAINING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create training pipeline (mode='train' includes churn labels)\n",
    "print(\"\\n1. Creating TRAINING pipeline...\")\n",
    "train_pipeline = create_feature_pipeline(\n",
    "    cutoff_date=pd.to_datetime('2099-12-31'),  # Use all data for training\n",
    "    mode='train',\n",
    "    window_days=10\n",
    ")\n",
    "\n",
    "# 2. Create temporal raw train/validation split (leak-free)\n",
    "print(\"\\n2. Creating temporal raw train/validation split...\")\n",
    "cutoff = pd.Timestamp(\"2018-11-01\")\n",
    "raw_train = df_raw[pd.to_datetime(df_raw['time']).dt.normalize() < cutoff.normalize()]\n",
    "raw_val   = df_raw[pd.to_datetime(df_raw['time']).dt.normalize() >= cutoff.normalize()]\n",
    "print(f\"   Train raw rows: {len(raw_train):,}\")\n",
    "print(f\"   Val raw rows:   {len(raw_val):,}\")\n",
    "\n",
    "# 3. Transform raw data to features (with raw_df passed)\n",
    "print(\"\\n3. Transforming raw data to features (with raw_df passed)...\")\n",
    "train_feat = train_pipeline.fit_transform(\n",
    "    raw_train,\n",
    "    accumulated__raw_df=raw_train,\n",
    "    page_interactions__raw_df=raw_train,\n",
    "    churn_target__raw_df=raw_train,\n",
    " )\n",
    "val_feat = train_pipeline.transform(raw_val)  # uses state (including raw_df_) learned from training fit\n",
    "print(f\"   Train features shape: {train_feat.shape}\")\n",
    "print(f\"   Val features shape:   {val_feat.shape}\")\n",
    "\n",
    "# 4. Prepare train/validation feature matrices and targets\n",
    "df_train = train_feat.copy()\n",
    "df_val = val_feat.copy()\n",
    "\n",
    "exclude_cols = ['userId', 'date', 'churn_status']\n",
    "feature_cols = [col for col in df_train.columns if col not in exclude_cols]\n",
    "\n",
    "X_train = df_train[feature_cols]\n",
    "y_train = df_train['churn_status']\n",
    "X_val = df_val[feature_cols]\n",
    "y_val = df_val['churn_status']\n",
    "\n",
    "print(f\"\\n4. Feature matrix prepared:\")\n",
    "print(f\"   Features: {len(feature_cols)}\")\n",
    "print(f\"   Train churn rate: {y_train.mean():.2%}\")\n",
    "print(f\"   Val churn rate: {y_val.mean():.2%}\")\n",
    "\n",
    "# 5. Build and train model\n",
    "print(\"\\n5. Training LightGBM model...\")\n",
    "\n",
    "# Calculate class weights\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_count = (y_train == 1).sum()\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "\n",
    "# Identify feature types\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Build model pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "        ]\n",
    "    )),\n",
    "    ('classifier', lgb.LGBMClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        n_jobs=-1,\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1\n",
    "    ))\n",
    "])\n",
    "\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(f\"   ‚úì Model trained (scale_pos_weight={scale_pos_weight:.2f})\")\n",
    "\n",
    "# 6. Evaluate on validation set and optimize threshold\n",
    "print(\"\\n6. Evaluating on validation set...\")\n",
    "y_val_pred_proba = model_pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Test different thresholds\n",
    "thresholds = np.arange(0.05, 0.95, 0.01)\n",
    "balanced_accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_val_pred_proba >= threshold).astype(int)\n",
    "    bal_acc = balanced_accuracy_score(y_val, y_pred_threshold)\n",
    "    balanced_accuracies.append(bal_acc)\n",
    "\n",
    "# Find optimal threshold\n",
    "optimal_idx = np.argmax(balanced_accuracies)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "optimal_bal_acc = balanced_accuracies[optimal_idx]\n",
    "\n",
    "print(f\"\\n   Optimal threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"   Validation balanced accuracy: {optimal_bal_acc:.4f}\")\n",
    "print(f\"   Validation ROC-AUC: {roc_auc_score(y_val, y_val_pred_proba):.4f}\")\n",
    "\n",
    "# Show classification report with optimal threshold\n",
    "y_val_pred_optimal = (y_val_pred_proba >= optimal_threshold).astype(int)\n",
    "print(f\"\\n   Classification Report (threshold={optimal_threshold:.2f}):\")\n",
    "print(classification_report(y_val, y_val_pred_optimal, target_names=['No Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c261bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING KAGGLE SUBMISSION\n",
      "================================================================================\n",
      "\n",
      "1. Loading Kaggle test data...\n",
      "   Test data shape: (4393179, 11)\n",
      "   Date range: 2018-10-01 00:00:06 to 2018-11-20 00:00:00\n",
      "   Unique users: 2904\n",
      "\n",
      "2. Re-fitting cumulative features on combined train+test...\n",
      "\n",
      "3. Creating prediction pipeline from training-fitted transformers...\n",
      "\n",
      "4. Transforming test data with training-fitted pipeline...\n",
      "BasicEventAggregator: Aggregating events to user-day level...\n",
      "  Output shape: (148104, 28)\n",
      "  Features: ['About', 'Add Friend', 'Add to Playlist', 'Downgrade', 'Error', 'Help', 'Home', 'Login', 'Logout', 'NextSong', 'Register', 'Roll Advert', 'Save Settings', 'Settings', 'Submit Downgrade', 'Submit Registration', 'Submit Upgrade', 'Thumbs Down', 'Thumbs Up', 'Upgrade', 'event_count', 'session_count', 'avg_session_length', 'events_per_session', 'level', 'days_since_registration']\n",
      "RollingAverageTransformerModular: Computing 7d rolling averages...\n",
      "RollingAverageTransformerModular: Computing 14d rolling averages...\n",
      "TrendFeaturesTransformer: Computing trend features...\n",
      "AccumulatedFeaturesTransformer: Computing cumulative features...\n",
      "  Added: accumulated_errors, accumulated_unique_artists\n",
      "PageInteractionTransformer: Computing page interaction features...\n",
      "  Added: []\n",
      "FeaturePreprocessor: Final preprocessing...\n",
      "   Test features shape: (148104, 48)\n",
      "\n",
      "5. Extracting last observation per user...\n",
      "   Submission samples: 2,904\n",
      "\n",
      "6. Aligning features with training schema...\n",
      "   Added missing feature: Add Friend_x\n",
      "   Added missing feature: Add to Playlist_x\n",
      "   Added missing feature: Cancel_x\n",
      "   Added missing feature: Cancellation Confirmation_x\n",
      "   Added missing feature: Downgrade_x\n",
      "   Added missing feature: Error_x\n",
      "   Added missing feature: Logout_x\n",
      "   Added missing feature: NextSong_x\n",
      "   Added missing feature: Roll Advert_x\n",
      "   Added missing feature: Submit Downgrade_x\n",
      "   Added missing feature: Submit Upgrade_x\n",
      "   Added missing feature: Thumbs Down_x\n",
      "   Added missing feature: Thumbs Up_x\n",
      "   Added missing feature: Upgrade_x\n",
      "   Added missing feature: Add Friend_y\n",
      "   Added missing feature: Add to Playlist_y\n",
      "   Added missing feature: Cancel_y\n",
      "   Added missing feature: Cancellation Confirmation_y\n",
      "   Added missing feature: Downgrade_y\n",
      "   Added missing feature: Error_y\n",
      "   Added missing feature: Logout_y\n",
      "   Added missing feature: NextSong_y\n",
      "   Added missing feature: Roll Advert_y\n",
      "   Added missing feature: Submit Downgrade_y\n",
      "   Added missing feature: Submit Upgrade_y\n",
      "   Added missing feature: Thumbs Down_y\n",
      "   Added missing feature: Thumbs Up_y\n",
      "   Added missing feature: Upgrade_y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/opt/anaconda3/envs/ds/lib/python3.10/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Features aligned: 59 features\n",
      "\n",
      "7. Making predictions...\n",
      "   Using threshold: 0.43\n",
      "   Predicted churn rate: 42.46%\n",
      "\n",
      "================================================================================\n",
      "‚úì SUBMISSION GENERATED (LEAK-FREE!)\n",
      "================================================================================\n",
      "\n",
      "File: /Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn/data/submission_ndl.csv\n",
      "Shape: (2904, 2)\n",
      "Users: 2,904\n",
      "\n",
      "Target distribution:\n",
      "target\n",
      "0    1671\n",
      "1    1233\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Predicted churn rate: 42.46%\n",
      "\n",
      "‚úì Ready for Kaggle submission!\n",
      "\n",
      "Model Performance (validation):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/opt/anaconda3/envs/ds/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROC-AUC: 0.7448\n",
      "  Balanced Accuracy: 0.6875\n",
      "  Optimal threshold: 0.43\n",
      "\n",
      "üîí NO DATA LEAKAGE:\n",
      "  ‚úì Using .transform() only (no refitting on test)\n",
      "  ‚úì All transformers fitted on training data\n",
      "  ‚úì Only cumulative features use combined data (valid)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GENERATE KAGGLE SUBMISSION (LEAK-FREE - CORRECTED!)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING KAGGLE SUBMISSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Load Kaggle test data\n",
    "print(\"\\n1. Loading Kaggle test data...\")\n",
    "df_test_raw = pd.read_parquet(root + '/data/test.parquet')\n",
    "\n",
    "# Clean up test data (same as training)\n",
    "object_cols_test = df_test_raw.select_dtypes(include=\"object\").columns\n",
    "df_test_raw[object_cols_test] = df_test_raw[object_cols_test].astype(\"category\")\n",
    "df_test_raw = df_test_raw.drop(\n",
    "    columns=['gender', 'firstName', 'lastName', 'location', 'userAgent', 'status', 'auth', 'method']\n",
    ")\n",
    "\n",
    "print(f\"   Test data shape: {df_test_raw.shape}\")\n",
    "print(f\"   Date range: {pd.to_datetime(df_test_raw['time']).min()} to {pd.to_datetime(df_test_raw['time']).max()}\")\n",
    "print(f\"   Unique users: {df_test_raw['userId'].nunique()}\")\n",
    "\n",
    "# 2. Re-fit ONLY cumulative features on combined train+test data\n",
    "print(\"\\n2. Re-fitting cumulative features on combined train+test...\")\n",
    "df_combined = pd.concat([df_raw, df_test_raw], ignore_index=True)\n",
    "\n",
    "# Re-fit only the transformers that need full user history\n",
    "if 'accumulated' in dict(train_pipeline.steps):\n",
    "    train_pipeline.named_steps['accumulated'].fit(None, raw_df=df_combined)\n",
    "if 'page_interactions' in dict(train_pipeline.steps):\n",
    "    train_pipeline.named_steps['page_interactions'].fit(None, raw_df=df_combined)\n",
    "\n",
    "# 3. Create prediction pipeline by removing churn_target from trained pipeline\n",
    "print(\"\\n3. Creating prediction pipeline from training-fitted transformers...\")\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Build pipeline WITHOUT churn_target transformer\n",
    "predict_steps = [\n",
    "    (name, transformer) for name, transformer in train_pipeline.steps\n",
    "    if name != 'churn_target'\n",
    "]\n",
    "predict_pipeline = Pipeline(predict_steps)\n",
    "\n",
    "# 4. Transform test data (using FITTED transformers from training!)\n",
    "print(\"\\n4. Transforming test data with training-fitted pipeline...\")\n",
    "df_test_features = predict_pipeline.transform(df_test_raw)  # ‚Üê TRANSFORM ONLY - NO REFITTING!\n",
    "print(f\"   Test features shape: {df_test_features.shape}\")\n",
    "\n",
    "# 5. Get last observation per user\n",
    "print(\"\\n5. Extracting last observation per user...\")\n",
    "df_test_features['date'] = pd.to_datetime(df_test_features['date'])\n",
    "last_user_data = df_test_features.sort_values('date').groupby('userId').tail(1).reset_index(drop=True)\n",
    "print(f\"   Submission samples: {len(last_user_data):,}\")\n",
    "\n",
    "# 6. Align features with training (add missing columns, ensure same order)\n",
    "print(\"\\n6. Aligning features with training schema...\")\n",
    "for col in feature_cols:\n",
    "    if col not in last_user_data.columns:\n",
    "        last_user_data[col] = 0\n",
    "        print(f\"   Added missing feature: {col}\")\n",
    "\n",
    "X_submission = last_user_data[feature_cols]\n",
    "print(f\"   ‚úì Features aligned: {len(feature_cols)} features\")\n",
    "\n",
    "# 7. Make predictions with optimal threshold\n",
    "print(\"\\n7. Making predictions...\")\n",
    "y_pred_proba = model_pipeline.predict_proba(X_submission)[:, 1]\n",
    "y_pred = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"   Using threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"   Predicted churn rate: {y_pred.mean():.2%}\")\n",
    "\n",
    "# 8. Create and save submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': last_user_data['userId'].astype(int).values,\n",
    "    'target': y_pred\n",
    "})\n",
    "\n",
    "output_path = root + '/data/submission_ndl.csv'\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì SUBMISSION GENERATED (LEAK-FREE!)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nFile: {output_path}\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"Users: {len(submission):,}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(submission['target'].value_counts())\n",
    "print(f\"\\nPredicted churn rate: {submission['target'].mean():.2%}\")\n",
    "print(f\"\\n‚úì Ready for Kaggle submission!\")\n",
    "print(f\"\\nModel Performance (validation):\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_val, y_val_pred_proba):.4f}\")\n",
    "print(f\"  Balanced Accuracy: {optimal_bal_acc:.4f}\")\n",
    "print(f\"  Optimal threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"\\nüîí NO DATA LEAKAGE:\")\n",
    "print(f\"  ‚úì Using .transform() only (no refitting on test)\")\n",
    "print(f\"  ‚úì All transformers fitted on training data\")\n",
    "print(f\"  ‚úì Only cumulative features use combined data (valid)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9d58d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUBMISSION FILE VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "1. File Structure:\n",
      "   Columns: ['id', 'target']\n",
      "   Shape: (2904, 2)\n",
      "   ‚úÖ Format correct: True\n",
      "\n",
      "2. Data Quality:\n",
      "   Unique users: 2904\n",
      "   Duplicates: 0\n",
      "   Missing values: 0\n",
      "   ‚úÖ No duplicates: True\n",
      "   ‚úÖ No missing: True\n",
      "\n",
      "3. Target Values:\n",
      "   Unique values: [0, 1]\n",
      "   ‚úÖ Binary (0/1): True\n",
      "\n",
      "4. Sample Predictions:\n",
      "        id  target\n",
      "0  1995115       0\n",
      "1  1993285       1\n",
      "2  1979129       1\n",
      "3  1997769       0\n",
      "4  1997880       1\n",
      "5  1985914       0\n",
      "6  1987068       0\n",
      "7  1988412       1\n",
      "8  1994524       1\n",
      "9  1988592       1\n",
      "\n",
      "5. Probability Distribution:\n",
      "   (0.0, 0.1]:     9 (  0.3%) \n",
      "   (0.1, 0.2]:   142 (  4.9%) ‚ñà‚ñà\n",
      "   (0.2, 0.3]:   729 ( 25.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   (0.3, 0.4]:   630 ( 21.7%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   (0.4, 0.5]:   479 ( 16.5%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   (0.5, 0.6]:   550 ( 18.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   (0.6, 0.7]:   361 ( 12.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   (0.7, 0.8]:     4 (  0.1%) \n",
      "   (0.8, 0.9]:     0 (  0.0%) \n",
      "   (0.9, 1.0]:     0 (  0.0%) \n",
      "\n",
      "================================================================================\n",
      "‚úÖ SUBMISSION FILE VERIFIED - READY FOR UPLOAD\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERIFY SUBMISSION FILE\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"SUBMISSION FILE VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read the generated submission\n",
    "submission_check = pd.read_csv(output_path)\n",
    "\n",
    "print(\"\\n1. File Structure:\")\n",
    "print(f\"   Columns: {list(submission_check.columns)}\")\n",
    "print(f\"   Shape: {submission_check.shape}\")\n",
    "print(f\"   ‚úÖ Format correct: {list(submission_check.columns) == ['id', 'target']}\")\n",
    "\n",
    "print(\"\\n2. Data Quality:\")\n",
    "print(f\"   Unique users: {submission_check['id'].nunique()}\")\n",
    "print(f\"   Duplicates: {submission_check['id'].duplicated().sum()}\")\n",
    "print(f\"   Missing values: {submission_check.isnull().sum().sum()}\")\n",
    "print(f\"   ‚úÖ No duplicates: {submission_check['id'].duplicated().sum() == 0}\")\n",
    "print(f\"   ‚úÖ No missing: {submission_check.isnull().sum().sum() == 0}\")\n",
    "\n",
    "print(\"\\n3. Target Values:\")\n",
    "print(f\"   Unique values: {sorted(submission_check['target'].unique())}\")\n",
    "print(f\"   ‚úÖ Binary (0/1): {set(submission_check['target'].unique()) == {0, 1}}\")\n",
    "\n",
    "print(\"\\n4. Sample Predictions:\")\n",
    "print(submission_check.head(10))\n",
    "\n",
    "print(\"\\n5. Probability Distribution:\")\n",
    "bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "prob_dist = pd.cut(y_pred_proba, bins=bins).value_counts().sort_index()\n",
    "for interval, count in prob_dist.items():\n",
    "    pct = count / len(y_pred_proba) * 100\n",
    "    bar = '‚ñà' * int(pct / 2)\n",
    "    print(f\"   {interval}: {count:5d} ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ SUBMISSION FILE VERIFIED - READY FOR UPLOAD\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5b06aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RawDataSplitter: Filtered to 5,130,187 events (<= 2018-10-15)\n",
      "BasicEventAggregator: Aggregating events to user-day level...\n",
      "  Output shape: (287100, 27)\n",
      "  Features: ['About', 'Add Friend', 'Add to Playlist', 'Cancel', 'Cancellation Confirmation', 'Downgrade', 'Error', 'Help', 'Home', 'Logout', 'NextSong', 'Roll Advert', 'Save Settings', 'Settings', 'Submit Downgrade', 'Submit Upgrade', 'Thumbs Down', 'Thumbs Up', 'Upgrade', 'event_count', 'session_count', 'avg_session_length', 'events_per_session', 'level', 'days_since_registration']\n",
      "RollingAverageTransformerModular: Computing 7d rolling averages...\n",
      "RollingAverageTransformerModular: Computing 14d rolling averages...\n",
      "TrendFeaturesTransformer: Computing trend features...\n",
      "AccumulatedFeaturesTransformer: Computing cumulative features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn/src/preprocessing.py:752: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  errors_per_day = df_errors.groupby([self.user_col, 'date']).size().reset_index(name='daily_errors')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added: accumulated_errors, accumulated_unique_artists\n",
      "PageInteractionTransformer: Computing page interaction features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn/src/preprocessing.py:858: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  page_counts = df_pages.groupby([self.user_col, 'date', self.page_col]).size().unstack(fill_value=0).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added: []\n",
      "FeaturePreprocessor: Final preprocessing...\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import src.preprocessing\n",
    "reload(src.preprocessing)\n",
    "\n",
    "from src.preprocessing import create_feature_pipeline\n",
    "\n",
    "pipe = create_feature_pipeline(cutoff_date='2018-10-15', mode='predict')\n",
    "\n",
    "out = pipe.fit_transform(\n",
    "    df_raw,\n",
    "    accumulated__raw_df=df_raw,\n",
    "    page_interactions__raw_df=df_raw,\n",
    ")\n",
    "\n",
    "assert out['date'].max() <= pd.Timestamp('2018-10-15')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
