{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cbb3974",
   "metadata": {},
   "source": [
    "# Churn Prediction Model - Production Notebook\n",
    "\n",
    "## Overview\n",
    "This notebook implements a churn prediction model using LightGBM with the following pipeline:\n",
    "\n",
    "### Feature Set (36 features)\n",
    "- **Removed**: 5 raw event counts (Add Friend, Add to Playlist, Error, Thumbs Down, Thumbs Up)\n",
    "- **Kept**: \n",
    "  - Rolling averages (7-day and 14-day windows)\n",
    "  - Trend features (comparison of 7d vs 14d)\n",
    "  - Page type counts\n",
    "  - User metadata\n",
    "  \n",
    "### Workflow\n",
    "1. **Data Loading & Preprocessing**: Load and clean training data\n",
    "2. **Feature Engineering**: Create rolling averages and trend features\n",
    "3. **Model Training**: Train LightGBM with optimized hyperparameters\n",
    "4. **Threshold Optimization**: Find optimal classification threshold\n",
    "5. **Submission Generation**: Generate final predictions for test data\n",
    "\n",
    "### Model Configuration\n",
    "- **Algorithm**: LightGBM (Gradient Boosting)\n",
    "- **Class Balancing**: Enabled via scale_pos_weight\n",
    "- **Validation**: Time-series split (70/30)\n",
    "- **Optimization**: Balanced accuracy maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import src.preprocessing\n",
    "from importlib import reload\n",
    "reload(src.preprocessing)\n",
    "\n",
    "from src.preprocessing import (\n",
    "    aggregate_user_day_activity, \n",
    "    add_rolling_averages,\n",
    "    compute_cancellation_batch\n",
    ")\n",
    "\n",
    "# Import sklearn components\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancellationTargetTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Efficiently computes cancellation targets using vectorized operations.\n",
    "    Must be provided with raw_df during __init__.\n",
    "    \"\"\"\n",
    "    def __init__(self, window_days=10, raw_df=None):\n",
    "        self.window_days = window_days\n",
    "        self.raw_df = raw_df\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.raw_df is None:\n",
    "            raise ValueError(\"raw_df must be provided\")\n",
    "        \n",
    "        print(f\"Computing churn targets (vectorized, window={self.window_days}d)...\")\n",
    "        \n",
    "        # Use efficient batch computation\n",
    "        churn_targets = compute_cancellation_batch(\n",
    "            self.raw_df,\n",
    "            X,\n",
    "            window_days=self.window_days\n",
    "        )\n",
    "        \n",
    "        # Merge with X\n",
    "        X_copy = X.copy()\n",
    "        X_copy['date'] = pd.to_datetime(X_copy['date'])\n",
    "        churn_targets['date'] = pd.to_datetime(churn_targets['date'])\n",
    "        X_copy['userId'] = X_copy['userId'].astype(int)\n",
    "        churn_targets['userId'] = churn_targets['userId'].astype(int)\n",
    "        \n",
    "        result = X_copy.merge(churn_targets, on=['userId', 'date'], how='left')\n",
    "        \n",
    "        print(f\"Churn status distribution:\\n{result['churn_status'].value_counts()}\")\n",
    "        return result\n",
    "\n",
    "\n",
    "class RollingAverageTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes rolling average features.\"\"\"\n",
    "    def __init__(self, columns=None, window_days=7):\n",
    "        self.columns = columns if columns is not None else ['NextSong']\n",
    "        self.window_days = window_days\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(f\"Computing rolling averages (window={self.window_days}d)...\")\n",
    "        return add_rolling_averages(X, columns=self.columns, n=self.window_days)\n",
    "\n",
    "\n",
    "class ThumbsRatioTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes thumbs ratio from rolling averages.\"\"\"\n",
    "    def __init__(self, window_days=7):\n",
    "        self.window_days = window_days\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        up_col = f'thumbs_up_avg_{self.window_days}d'\n",
    "        down_col = f'thumbs_down_avg_{self.window_days}d'\n",
    "        ratio_col = f'thumbs_ratio_{self.window_days}d'\n",
    "        \n",
    "        if up_col in X_copy.columns and down_col in X_copy.columns:\n",
    "            denominator = X_copy[up_col] + X_copy[down_col]\n",
    "            X_copy[ratio_col] = X_copy[up_col] / denominator.replace(0, np.nan)\n",
    "            X_copy[ratio_col] = X_copy[ratio_col].fillna(0)\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "\n",
    "class TrendFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Creates trend features by comparing short-term (7d) vs long-term (14d) averages.\"\"\"\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns if columns is not None else ['NextSong']\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # For each column, compute trend (7d avg / 14d avg - 1)\n",
    "        # Positive = increasing activity, Negative = decreasing activity\n",
    "        for col in self.columns:\n",
    "            col_7d = f'{col.lower().replace(\" \", \"_\")}_avg_7d'\n",
    "            col_14d = f'{col.lower().replace(\" \", \"_\")}_avg_14d'\n",
    "            trend_col = f'{col.lower().replace(\" \", \"_\")}_trend'\n",
    "            \n",
    "            if col_7d in X_copy.columns and col_14d in X_copy.columns:\n",
    "                # Compute ratio: (7d / 14d) - 1\n",
    "                # This gives % change: positive = increasing, negative = decreasing\n",
    "                denominator = X_copy[col_14d].replace(0, np.nan)\n",
    "                X_copy[trend_col] = (X_copy[col_7d] / denominator) - 1\n",
    "                X_copy[trend_col] = X_copy[trend_col].fillna(0)\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "\n",
    "class FeaturePreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Handles type conversions and missing value imputation.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Convert level to binary\n",
    "        if 'level' in X_copy.columns:\n",
    "            X_copy['level'] = (X_copy['level'] == 'paid').astype(int)\n",
    "        \n",
    "        # Create user lifecycle feature: long-time user (30+ days) vs recent user\n",
    "        # if 'days_since_registration' in X_copy.columns:\n",
    "        #     X_copy['is_established_user'] = (X_copy['days_since_registration'] >= 30).astype(int)\n",
    "        \n",
    "        # Add weekend indicator\n",
    "        if 'date' in X_copy.columns:\n",
    "            X_copy['date'] = pd.to_datetime(X_copy['date'])\n",
    "            X_copy['is_weekend'] = (X_copy['date'].dt.dayofweek >= 5).astype(int)\n",
    "        \n",
    "        # Fill ratio columns with 0\n",
    "        ratio_cols = [col for col in X_copy.columns if 'ratio' in col.lower()]\n",
    "        for col in ratio_cols:\n",
    "            if col in X_copy.columns:\n",
    "                X_copy[col] = pd.to_numeric(X_copy[col], errors='coerce').fillna(0)\n",
    "        \n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca68f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn'\n",
    "df_raw = pd.read_parquet(root + '/data/train.parquet')\n",
    "\n",
    "# Clean up: convert object columns to category, drop unnecessary columns\n",
    "object_cols = df_raw.select_dtypes(include=\"object\").columns\n",
    "df_raw[object_cols] = df_raw[object_cols].astype(\"category\")\n",
    "df_raw = df_raw.drop(columns=['gender', 'firstName', 'lastName', 'location', 'userAgent', 'status', 'auth', 'method'])\n",
    "\n",
    "print(f\"Raw data shape: {df_raw.shape}\")\n",
    "print(f\"Date range: {pd.to_datetime(df_raw['time']).min()} to {pd.to_datetime(df_raw['time']).max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHECK FOR COLUMN MISMATCHES BETWEEN TRAIN AND TEST DATA\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARING TRAIN.PARQUET vs TEST.PARQUET COLUMNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load both raw datasets\n",
    "df_train_raw = pd.read_parquet(root + '/data/train.parquet')\n",
    "df_test_raw_check = pd.read_parquet(root + '/data/test.parquet')\n",
    "\n",
    "print(f\"\\nTrain data shape: {df_train_raw.shape}\")\n",
    "print(f\"Test data shape: {df_test_raw_check.shape}\")\n",
    "\n",
    "# Get column sets\n",
    "train_cols = set(df_train_raw.columns)\n",
    "test_cols = set(df_test_raw_check.columns)\n",
    "\n",
    "print(f\"\\nTrain columns count: {len(train_cols)}\")\n",
    "print(f\"Test columns count: {len(test_cols)}\")\n",
    "\n",
    "# Find differences\n",
    "cols_only_in_train = train_cols - test_cols\n",
    "cols_only_in_test = test_cols - train_cols\n",
    "common_cols = train_cols & test_cols\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"COLUMN DIFFERENCES:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if cols_only_in_train:\n",
    "    print(f\"\\n⚠️  Columns ONLY in TRAIN (not in test): {len(cols_only_in_train)}\")\n",
    "    for col in sorted(cols_only_in_train):\n",
    "        print(f\"   - {col}\")\n",
    "else:\n",
    "    print(\"\\n✓ No columns unique to train\")\n",
    "\n",
    "if cols_only_in_test:\n",
    "    print(f\"\\n⚠️  Columns ONLY in TEST (not in train): {len(cols_only_in_test)}\")\n",
    "    for col in sorted(cols_only_in_test):\n",
    "        print(f\"   - {col}\")\n",
    "else:\n",
    "    print(\"\\n✓ No columns unique to test\")\n",
    "\n",
    "print(f\"\\n✓ Common columns: {len(common_cols)}\")\n",
    "print(f\"   {sorted(common_cols)}\")\n",
    "\n",
    "# Check which columns we're currently dropping\n",
    "dropped_cols = ['gender', 'firstName', 'lastName', 'location', 'userAgent', 'status', 'auth', 'method']\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(f\"Currently dropping these columns: {dropped_cols}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58236d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to user-day level\n",
    "print(\"\\nAggregating events to user-day level...\")\n",
    "df_agg = aggregate_user_day_activity(df_raw)\n",
    "df_agg['userId'] = df_agg['userId'].astype(int)\n",
    "\n",
    "print(f\"Aggregated data shape: {df_agg.shape}\")\n",
    "print(f\"Date range: {df_agg['date'].min()} to {df_agg['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE REDUCTION: DROP RARE/LOW-SIGNAL PAGE TYPE FEATURES\n",
    "# ============================================================================\n",
    "# Final model keeps: 7d + 14d rolling windows, trend features, thumbs ratios\n",
    "# Only removes: 9 rare page type features that add noise\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE REDUCTION: DROPPING RARE/LOW-SIGNAL PAGE TYPES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define features to remove (9 rare/low-signal page types)\n",
    "features_to_remove = [\n",
    "    'About', 'Help', 'Settings', 'Save Settings', 'Roll Advert', \n",
    "    'Home', 'Logout', 'Submit Downgrade', 'Submit Upgrade'\n",
    "]\n",
    "\n",
    "# Check which features exist in the aggregated data\n",
    "existing_features_to_remove = [col for col in features_to_remove if col in df_agg.columns]\n",
    "\n",
    "print(f\"\\nOriginal aggregated data shape: {df_agg.shape}\")\n",
    "print(f\"Features to remove: {len(features_to_remove)} rare page types\")\n",
    "\n",
    "if existing_features_to_remove:\n",
    "    print(f\"  Removing: {existing_features_to_remove}\")\n",
    "    df_agg = df_agg.drop(columns=existing_features_to_remove)\n",
    "    print(f\"\\n✓ Removed {len(existing_features_to_remove)} rare/low-signal features\")\n",
    "    print(f\"✓ New data shape: {df_agg.shape}\")\n",
    "    print(f\"✓ KEEPING: 7d + 14d rolling windows, trend features, thumbs ratios\")\n",
    "else:\n",
    "    print(\"\\n✓ No features to remove (already absent from data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1131c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal train-test split\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEMPORAL TRAIN/TEST SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cutoff_date = pd.to_datetime('2018-11-01')\n",
    "df_agg['date'] = pd.to_datetime(df_agg['date'])\n",
    "\n",
    "df_train = df_agg[df_agg['date'] < cutoff_date].copy()\n",
    "df_test = df_agg[df_agg['date'] >= cutoff_date].copy()\n",
    "\n",
    "print(f\"Training set: {df_train.shape}\")\n",
    "print(f\"Test set: {df_test.shape}\")\n",
    "print(f\"Train dates: {df_train['date'].min()} to {df_train['date'].max()}\")\n",
    "print(f\"Test dates: {df_test['date'].min()} to {df_test['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cee51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BUILD AND APPLY FEATURE ENGINEERING PIPELINE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING PIPELINE (VECTORIZED & EFFICIENT)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define feature columns to track\n",
    "feature_cols_list = ['Add Friend', 'Add to Playlist', 'Thumbs Down', 'Thumbs Up', 'Error']\n",
    "\n",
    "# Create pipeline for training data\n",
    "print(\"\\n1. Transforming TRAINING data...\")\n",
    "train_pipeline = Pipeline([\n",
    "    ('churn_target', CancellationTargetTransformer(window_days=10, raw_df=df_raw)),\n",
    "    ('rolling_avg_7d', RollingAverageTransformer(columns=feature_cols_list, window_days=7)),\n",
    "    ('rolling_avg_14d', RollingAverageTransformer(columns=feature_cols_list, window_days=14)),\n",
    "    # ('thumbs_ratio_7d', ThumbsRatioTransformer(window_days=7)),\n",
    "    # ('thumbs_ratio_14d', ThumbsRatioTransformer(window_days=14)),\n",
    "    ('trend_features', TrendFeaturesTransformer(columns=feature_cols_list)),\n",
    "    ('preprocessor', FeaturePreprocessor()),\n",
    "])\n",
    "\n",
    "df_train_features = train_pipeline.fit_transform(df_train)\n",
    "\n",
    "print(f\"\\nTraining features shape: {df_train_features.shape}\")\n",
    "print(f\"Columns: {df_train_features.columns.tolist()[:10]}... (showing first 10)\")\n",
    "\n",
    "# Apply same pipeline to test data\n",
    "print(\"\\n2. Transforming TEST data...\")\n",
    "test_pipeline = Pipeline([\n",
    "    ('churn_target', CancellationTargetTransformer(window_days=10, raw_df=df_raw)),\n",
    "    ('rolling_avg_7d', RollingAverageTransformer(columns=feature_cols_list, window_days=7)),\n",
    "    ('rolling_avg_14d', RollingAverageTransformer(columns=feature_cols_list, window_days=14)),\n",
    "    # ('thumbs_ratio_7d', ThumbsRatioTransformer(window_days=7)),\n",
    "    # ('thumbs_ratio_14d', ThumbsRatioTransformer(window_days=14)),\n",
    "    ('trend_features', TrendFeaturesTransformer(columns=feature_cols_list)),\n",
    "    ('preprocessor', FeaturePreprocessor()),\n",
    "])\n",
    "\n",
    "df_test_features = test_pipeline.fit_transform(df_test)\n",
    "\n",
    "print(f\"Test features shape: {df_test_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed98e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXTRACT FEATURES AND TARGET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXTRACTING FEATURES AND TARGET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "exclude_cols = ['userId', 'date', 'churn_status', 'Cancel']\n",
    "feature_cols = [col for col in df_train_features.columns if col not in exclude_cols]\n",
    "\n",
    "X_train = df_train_features[feature_cols].copy()\n",
    "y_train = df_train_features['churn_status'].copy()\n",
    "\n",
    "X_test = df_test_features[feature_cols].copy()\n",
    "y_test = df_test_features['churn_status'].copy()\n",
    "\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  X_train shape: {X_train.shape}\")\n",
    "print(f\"  y_train shape: {y_train.shape}\")\n",
    "print(f\"  Churn rate: {y_train.mean():.4f}\")\n",
    "print(f\"  Churn distribution:\\n{y_train.value_counts()}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  X_test shape: {X_test.shape}\")\n",
    "print(f\"  y_test shape: {y_test.shape}\")\n",
    "print(f\"  Churn rate: {y_test.mean():.4f}\")\n",
    "print(f\"  Churn distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b538ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SKLEARN PREPROCESSING + MODEL TRAINING PIPELINE (BASELINE PARAMS)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BUILDING FINAL PIPELINE: PREPROCESSING + LightGBM (BASELINE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify feature types\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {len(numeric_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Calculate scale_pos_weight to handle class imbalance\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_count = (y_train == 1).sum()\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "print(f\"\\nClass imbalance adjustment:\")\n",
    "print(f\"  Negative class (no churn): {neg_count}\")\n",
    "print(f\"  Positive class (churn): {pos_count}\")\n",
    "print(f\"  Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Use baseline hyperparameters\n",
    "baseline_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "}\n",
    "\n",
    "print(f\"\\nUsing baseline hyperparameters:\")\n",
    "for param, value in baseline_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Full pipeline with baseline hyperparameters\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lgb.LGBMClassifier(\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_jobs=-1,  # Use all processors\n",
    "        **baseline_params\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "y_pred_proba = model_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}')\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(f'Balanced Accuracy: {balanced_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b96293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIMIZE CLASSIFICATION THRESHOLD\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test different thresholds\n",
    "thresholds = np.arange(0.05, 0.95, 0.01)\n",
    "balanced_accuracies = []\n",
    "\n",
    "print(\"\\nTesting thresholds from 0.05 to 0.95...\")\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "    bal_acc = balanced_accuracy_score(y_test, y_pred_threshold)\n",
    "    balanced_accuracies.append(bal_acc)\n",
    "\n",
    "# Find optimal threshold\n",
    "optimal_idx = np.argmax(balanced_accuracies)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "optimal_bal_acc = balanced_accuracies[optimal_idx]\n",
    "\n",
    "print(f\"\\n✓ Optimization complete!\")\n",
    "print(f\"\\nDefault threshold (0.5):\")\n",
    "print(f\"  Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "print(f\"\\nOptimal threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"  Balanced Accuracy: {optimal_bal_acc:.4f}\")\n",
    "print(f\"  Improvement: +{(optimal_bal_acc - balanced_acc) * 100:.2f}%\")\n",
    "\n",
    "# Show predictions with optimal threshold\n",
    "y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"RESULTS WITH OPTIMAL THRESHOLD ({optimal_threshold:.2f})\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred_optimal))\n",
    "print(f'ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}')\n",
    "print(f'Balanced Accuracy: {optimal_bal_acc:.4f}')\n",
    "\n",
    "# Visualize threshold vs balanced accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, balanced_accuracies, linewidth=2)\n",
    "plt.axvline(optimal_threshold, color='red', linestyle='--', label=f'Optimal threshold: {optimal_threshold:.2f}')\n",
    "plt.axvline(0.5, color='gray', linestyle='--', alpha=0.5, label='Default threshold: 0.50')\n",
    "plt.axhline(balanced_acc, color='gray', linestyle=':', alpha=0.5)\n",
    "plt.xlabel('Classification Threshold', fontsize=12)\n",
    "plt.ylabel('Balanced Accuracy', fontsize=12)\n",
    "plt.title('Threshold Optimization for Balanced Accuracy', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Threshold optimization visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE REDUCED FEATURE SET (Remove raw event counts)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CREATING REDUCED FEATURE SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Remove raw event count features (keeping only derived features)\n",
    "raw_features_to_remove = ['Add Friend', 'Add to Playlist', 'Error', 'Thumbs Down', 'Thumbs Up']\n",
    "\n",
    "print(f\"\\nRemoving {len(raw_features_to_remove)} raw event count features:\")\n",
    "for feat in raw_features_to_remove:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nKEEPING all derived features:\")\n",
    "print(f\"  - Rolling averages (7d + 14d)\")\n",
    "print(f\"  - Trend features (7d vs 14d comparison)\")\n",
    "\n",
    "# Create reduced feature set\n",
    "X_train_reduced = X_train.drop(columns=raw_features_to_remove, errors='ignore')\n",
    "X_test_reduced = X_test.drop(columns=raw_features_to_remove, errors='ignore')\n",
    "\n",
    "print(f\"\\nOriginal features: {X_train.shape[1]}\")\n",
    "print(f\"Reduced features: {X_train_reduced.shape[1]}\")\n",
    "print(f\"Removed: {X_train.shape[1] - X_train_reduced.shape[1]} features\")\n",
    "\n",
    "# Retrain model with reduced features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RETRAINING MODEL WITH REDUCED FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "numeric_features_reduced = X_train_reduced.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features_reduced = X_train_reduced.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "preprocessor_reduced = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features_reduced),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features_reduced)\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_pipeline_reduced = Pipeline([\n",
    "    ('preprocessor', preprocessor_reduced),\n",
    "    ('classifier', lgb.LGBMClassifier(\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_jobs=-1,\n",
    "        **baseline_params\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "model_pipeline_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "y_pred_reduced = model_pipeline_reduced.predict(X_test_reduced)\n",
    "y_pred_proba_reduced = model_pipeline_reduced.predict_proba(X_test_reduced)[:, 1]\n",
    "\n",
    "balanced_acc_reduced = balanced_accuracy_score(y_test, y_pred_reduced)\n",
    "roc_auc_reduced = roc_auc_score(y_test, y_pred_proba_reduced)\n",
    "\n",
    "print(f\"\\nResults (default threshold 0.5):\")\n",
    "print(f\"  Balanced Accuracy: {balanced_acc_reduced:.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_reduced:.4f}\")\n",
    "\n",
    "# Optimize threshold\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMIZING THRESHOLD\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "balanced_accuracies_reduced = []\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_proba_reduced >= threshold).astype(int)\n",
    "    bal_acc = balanced_accuracy_score(y_test, y_pred_threshold)\n",
    "    balanced_accuracies_reduced.append(bal_acc)\n",
    "\n",
    "optimal_idx_reduced = np.argmax(balanced_accuracies_reduced)\n",
    "optimal_threshold_reduced = thresholds[optimal_idx_reduced]\n",
    "optimal_bal_acc_reduced = balanced_accuracies_reduced[optimal_idx_reduced]\n",
    "\n",
    "print(f\"\\nOptimal threshold: {optimal_threshold_reduced:.2f}\")\n",
    "print(f\"Balanced Accuracy: {optimal_bal_acc_reduced:.4f}\")\n",
    "print(f\"Improvement over default: +{(optimal_bal_acc_reduced - balanced_acc_reduced) * 100:.2f}%\")\n",
    "\n",
    "# Compare with full feature model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Full model ({X_train.shape[1]} features with raw counts):\")\n",
    "print(f\"  Balanced Accuracy: {optimal_bal_acc:.4f}\")\n",
    "print(f\"\\nReduced model ({X_train_reduced.shape[1]} features without raw counts):\")\n",
    "print(f\"  Balanced Accuracy: {optimal_bal_acc_reduced:.4f}\")\n",
    "print(f\"  Change: {(optimal_bal_acc_reduced - optimal_bal_acc):+.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Reduced model ready for submission generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GENERATE FINAL SUBMISSION CSV\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING FINAL SUBMISSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nMODEL CONFIGURATION:\")\n",
    "print(f\"  - Features: {X_train_reduced.shape[1]} (removed 5 raw event counts)\")\n",
    "print(f\"  - Optimal threshold: {optimal_threshold_reduced:.2f}\")\n",
    "print(f\"  - Balanced Accuracy: {optimal_bal_acc_reduced:.4f}\")\n",
    "print(f\"  - Model: LightGBM with class balancing\")\n",
    "\n",
    "# Load and process test data\n",
    "print(\"\\n1. Loading test data...\")\n",
    "df_test_raw_final = pd.read_parquet(root + '/data/test.parquet')\n",
    "\n",
    "# Clean up test data\n",
    "object_cols_test = df_test_raw_final.select_dtypes(include=\"object\").columns\n",
    "df_test_raw_final[object_cols_test] = df_test_raw_final[object_cols_test].astype(\"category\")\n",
    "df_test_raw_final = df_test_raw_final.drop(columns=['gender', 'firstName', 'lastName', 'location', 'userAgent', 'status', 'auth', 'method'])\n",
    "\n",
    "print(f\"   Test raw data shape: {df_test_raw_final.shape}\")\n",
    "\n",
    "# Aggregate to user-day level\n",
    "print(\"\\n2. Aggregating test data to user-day level...\")\n",
    "df_test_agg_final = aggregate_user_day_activity(df_test_raw_final)\n",
    "df_test_agg_final['userId'] = df_test_agg_final['userId'].astype(int)\n",
    "df_test_agg_final['date'] = pd.to_datetime(df_test_agg_final['date'])\n",
    "\n",
    "max_date_final = df_test_agg_final['date'].max()\n",
    "print(f\"   Test aggregated shape: {df_test_agg_final.shape}\")\n",
    "print(f\"   Unique users: {df_test_agg_final['userId'].nunique()}\")\n",
    "print(f\"   Max date: {max_date_final.date()}\")\n",
    "\n",
    "# Compute rolling features (7d + 14d)\n",
    "print(\"\\n3. Computing rolling average features (7d + 14d)...\")\n",
    "feature_cols_list = ['Add Friend', 'Add to Playlist', 'Thumbs Down', 'Thumbs Up', 'Error']\n",
    "\n",
    "test_with_features = add_rolling_averages(df_test_agg_final, columns=feature_cols_list, n=7)\n",
    "test_with_features = add_rolling_averages(test_with_features, columns=feature_cols_list, n=14)\n",
    "\n",
    "# Compute trend features (7d vs 14d)\n",
    "print(\"\\n4. Computing trend features...\")\n",
    "for col in feature_cols_list:\n",
    "    col_7d = f'{col.lower().replace(\" \", \"_\")}_avg_7d'\n",
    "    col_14d = f'{col.lower().replace(\" \", \"_\")}_avg_14d'\n",
    "    trend_col = f'{col.lower().replace(\" \", \"_\")}_trend'\n",
    "    \n",
    "    if col_7d in test_with_features.columns and col_14d in test_with_features.columns:\n",
    "        denominator = test_with_features[col_14d].replace(0, np.nan)\n",
    "        test_with_features[trend_col] = (test_with_features[col_7d] / denominator) - 1\n",
    "        test_with_features[trend_col] = test_with_features[trend_col].fillna(0)\n",
    "\n",
    "# Apply preprocessing transformations\n",
    "print(\"\\n5. Applying preprocessing...\")\n",
    "if 'level' in test_with_features.columns:\n",
    "    test_with_features['level'] = (test_with_features['level'] == 'paid').astype(int)\n",
    "\n",
    "if 'date' in test_with_features.columns:\n",
    "    test_with_features['date'] = pd.to_datetime(test_with_features['date'])\n",
    "    test_with_features['is_weekend'] = (test_with_features['date'].dt.dayofweek >= 5).astype(int)\n",
    "\n",
    "# Get last row per user\n",
    "last_user_data = test_with_features.sort_values('date').groupby('userId').tail(1).reset_index(drop=True)\n",
    "print(f\"   Users in test: {len(last_user_data)}\")\n",
    "\n",
    "# Align features with training data\n",
    "print(\"\\n6. Aligning features with training data...\")\n",
    "feature_cols_reduced = [col for col in X_train_reduced.columns]\n",
    "\n",
    "for col in feature_cols_reduced:\n",
    "    if col not in last_user_data.columns:\n",
    "        print(f\"   - Adding missing column: {col}\")\n",
    "        last_user_data[col] = 0\n",
    "\n",
    "X_test_submission_final = last_user_data[feature_cols_reduced].copy()\n",
    "print(f\"   Final feature matrix: {X_test_submission_final.shape}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\n7. Making predictions...\")\n",
    "print(f\"   Using optimized threshold: {optimal_threshold_reduced:.2f}\")\n",
    "\n",
    "y_pred_proba_final = model_pipeline_reduced.predict_proba(X_test_submission_final)[:, 1]\n",
    "y_pred_final = (y_pred_proba_final >= optimal_threshold_reduced).astype(int)\n",
    "\n",
    "# Create submission\n",
    "submission_final = pd.DataFrame({\n",
    "    'id': last_user_data['userId'].astype(int).values,\n",
    "    'target': y_pred_final\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_path_final = root + '/data/submission_final.csv'\n",
    "submission_final.to_csv(output_path_final, index=False)\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"✓ SUBMISSION GENERATED\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"\\nFile: {output_path_final}\")\n",
    "print(f\"Shape: {submission_final.shape}\")\n",
    "print(f\"Users: {len(submission_final):,}\")\n",
    "print(f\"\\nPredicted churn rate: {submission_final['target'].mean():.2%}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(submission_final['target'].value_counts())\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"  - Total features: {X_train_reduced.shape[1]}\")\n",
    "print(f\"  - Removed: 5 raw event counts (Add Friend, Add to Playlist, Error, Thumbs Down, Thumbs Up)\")\n",
    "print(f\"  - Kept: All rolling windows (7d + 14d), trend features\")\n",
    "print(f\"  - Threshold: {optimal_threshold_reduced:.2f} (optimized)\")\n",
    "print(f\"  - Balanced accuracy: {optimal_bal_acc_reduced:.4f}\")\n",
    "print(f\"  - Prediction window: {(max_date_final + pd.Timedelta(days=1)).date()} to {(max_date_final + pd.Timedelta(days=10)).date()}\")\n",
    "print(f\"\\n✓ Ready for Kaggle submission!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
