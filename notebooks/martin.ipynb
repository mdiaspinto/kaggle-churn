{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc6bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn'\n",
    "df_raw = pd.read_parquet(root + '/data/train.parquet')\n",
    "unused = ['status', 'firstName', 'lastName', 'ts', 'method', 'auth', 'userAgent']\n",
    "df_raw.drop(columns=unused, inplace=True)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8008ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/Users/mdiaspinto/Documents/School/Python Data Science/Final Project/kaggle-churn'\n",
    "df_test = pd.read_parquet(root + '/data/test.parquet')\n",
    "df_test.drop(columns=unused, inplace=True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5887e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_builder(df: pd.DataFrame, cutoff_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build comprehensive user features based on historical activity up to a cutoff date.\n",
    "    \n",
    "    Creates aggregate features including session statistics, engagement metrics,\n",
    "    thumbs up/down percentages for recent sessions, and user registration info.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Raw event data containing user activity logs with columns:\n",
    "        userId, time, sessionId, level, registration, song, page, artist\n",
    "    cutoff_date : pd.Timestamp\n",
    "        The date to use as the observation cutoff. Only events before this date\n",
    "        are used to compute features.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Feature matrix with userId as index and engineered features as columns.\n",
    "        Features include session counts, engagement percentages, trends, and\n",
    "        user demographics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a slice of the dataframe up to the cutoff date and makes userId the index\n",
    "    df_slice = df[df['time'] < cutoff_date].copy()\n",
    "    idx = pd.Index(np.sort(df_slice['userId'].unique()), name='userId')\n",
    "    final_df = pd.DataFrame(index=idx)\n",
    "\n",
    "    # Get key features from users at cutoff date\n",
    "    user_group = df.groupby('userId')\n",
    "    final_df['level'] = user_group['level'].last().reindex(idx)\n",
    "    final_df['days_registered'] = \\\n",
    "        (cutoff_date.normalize() - user_group['registration'].min().reindex(idx).dt.normalize()).dt.days.astype(int)\n",
    "\n",
    "    # Group sessions and defines start and end for each one\n",
    "    session_group = df_slice.groupby(['userId', 'sessionId']).agg(\n",
    "        session_start=('time', 'min'),\n",
    "        session_end=('time', 'max'),\n",
    "        song_count=('song', 'count')\n",
    "    )\n",
    "\n",
    "    # Calculate session length in seconds\n",
    "    session_group['session_length'] = (\n",
    "    session_group['session_end'] - session_group['session_start']\n",
    "    ).dt.total_seconds()\n",
    "    \n",
    "    # Aggregate session statistics per user\n",
    "    session_stats = session_group.groupby('userId').agg(\n",
    "        num_sessions=('session_start', 'count'),\n",
    "        avg_songs_per_session=('song_count', 'mean'),\n",
    "        avg_session_length=('session_length', 'mean'),\n",
    "        days_since_last_session=('session_end', 'max'),\n",
    "    )\n",
    "\n",
    "    # Convert to days and handle NaT\n",
    "    session_stats['days_since_last_session'] = (\n",
    "        (cutoff_date - session_stats['days_since_last_session']).dt.days\n",
    "    )\n",
    "\n",
    "    # Convert to hours\n",
    "    session_stats['avg_session_length'] /= 3600\n",
    "\n",
    "    # Calculate proportion of activity on weekends\n",
    "    df_slice['day'] = df_slice['time'].dt.dayofweek\n",
    "    df_slice['weekend'] = df_slice['day'].isin([5, 6]).astype(int)\n",
    "    final_df['weekend_perc'] = (df_slice.groupby('userId')['weekend'].sum()\\\n",
    "        /df_slice.groupby('userId')['weekend'].count()).reindex(idx, fill_value=0)\n",
    "    final_df['weekend_perc'] *= 100\n",
    "\n",
    "    # Calculate proportion of weekend days in the target window\n",
    "    target_window = pd.date_range(start=cutoff_date + pd.Timedelta(days=1), periods=10)\n",
    "    weekend_window_perc = (target_window.dayofweek.isin([5, 6])).sum()\n",
    "    final_df['weekend_target_perc'] = weekend_window_perc * 10\n",
    "\n",
    "    # Build thumbs up and thumbs down features for last 5 and 10 sessions\n",
    "    for n_sessions in (5, 10):\n",
    "        # Find last N sessions per user\n",
    "        lastN = (\n",
    "            session_group.reset_index()[['userId', 'sessionId', 'session_end']]\n",
    "            .sort_values(['userId', 'session_end'])\n",
    "            .groupby('userId', as_index=False)\n",
    "            .tail(n_sessions)\n",
    "        )\n",
    "        lastN_keys = pd.MultiIndex.from_frame(lastN[['userId', 'sessionId']])\n",
    "\n",
    "        # Filter events to those sessions\n",
    "        df_lastN = (\n",
    "            df_slice.set_index(['userId', 'sessionId'])\n",
    "            .loc[lambda d: d.index.isin(lastN_keys)]\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Get user and page counts in last N sessions\n",
    "        page_group = (\n",
    "            df_lastN.groupby(['userId', 'page'])\n",
    "            .size()\n",
    "            .unstack()\n",
    "            .reindex(idx)\n",
    "            .fillna(0)\n",
    "        )\n",
    "\n",
    "        # Total songs in last N sessions\n",
    "        user_songs = (\n",
    "            session_group.loc[session_group.index.isin(lastN_keys), 'song_count']\n",
    "            .groupby(level=0)\n",
    "            .sum()\n",
    "            .reindex(idx, fill_value=0)\n",
    "        )\n",
    "        denom = user_songs.replace(0, 1)\n",
    "\n",
    "        suffix = f'_last{n_sessions}'\n",
    "\n",
    "        if n_sessions == 5:\n",
    "            # Calculate unique artists played in last 5 sessions\n",
    "            plays_lastN = df_lastN[df_lastN['page'] == 'NextSong']\n",
    "            unique_artists = (\n",
    "            plays_lastN.groupby('userId')['artist']\n",
    "            .nunique()\n",
    "            .reindex(idx, fill_value=0)\n",
    "            .astype(int)\n",
    "            )\n",
    "            final_df[f'unique_artists_last5'] = 100 * unique_artists / denom\n",
    "\n",
    "            # Calculate several feature counts for last 5 sessions\n",
    "            final_df['roll_advert_count_last5'] = (\n",
    "                page_group.get('Roll Advert', pd.Series(0, index=idx)).astype(int)\n",
    "            )\n",
    "            final_df['error_count_last5'] = (\n",
    "                page_group.get('Error', pd.Series(0, index=idx)).astype(int)\n",
    "            )\n",
    "            final_df['about_count_last5'] = (\n",
    "                page_group.get('About', pd.Series(0, index=idx)).astype(int)\n",
    "            )\n",
    "            final_df['add_playlist_count_last5'] = (\n",
    "                page_group.get('Add to Playlist', pd.Series(0, index=idx)).astype(int)\n",
    "            )\n",
    "\n",
    "            final_df['roll_advert_perc_last5'] = 100 * final_df['roll_advert_count_last5'] / denom\n",
    "            final_df['error_perc_last5'] = 100 * final_df['error_count_last5'] / denom\n",
    "            final_df['about_perc_last5'] = 100 * final_df['about_count_last5'] / denom\n",
    "            final_df['add_playlist_perc_last5'] = 100 * final_df['add_playlist_count_last5'] / denom\n",
    "            columns_drop = ['roll_advert_count_last5', 'error_count_last5', \\\n",
    "                            'about_count_last5', 'add_playlist_count_last5']\n",
    "            final_df.drop(columns = columns_drop, inplace=True)\n",
    "\n",
    "        final_df[f'thumbs_up_perc{suffix}'] = 100 * page_group.get('Thumbs Up', 0) / denom\n",
    "        final_df[f'thumbs_down_perc{suffix}'] = 100 * page_group.get('Thumbs Down', 0) / denom\n",
    "        final_df[f'thumbs_up_down_perc{suffix}'] = (\n",
    "            final_df[f'thumbs_up_perc{suffix}'] - final_df[f'thumbs_down_perc{suffix}']\n",
    "        )\n",
    "\n",
    "    # Calculate trends between last 5 and last 10 sessions\n",
    "    final_df['thumbs_up_trend'] = final_df['thumbs_up_perc_last5'] - final_df['thumbs_up_perc_last10']\n",
    "    final_df['thumbs_down_trend'] = final_df['thumbs_down_perc_last5'] - final_df['thumbs_down_perc_last10']\n",
    "    final_df['thumbs_up_down_trend'] = final_df['thumbs_up_perc_last5'] - final_df['thumbs_up_perc_last10']\n",
    "\n",
    "    # Calculate how long user has been premium on a proportion of observed activity\n",
    "    premium_count = (\n",
    "    df_slice.loc[df_slice[\"level\"].eq(\"paid\")]\n",
    "    .groupby(\"userId\")\n",
    "    .size()\n",
    "    .reindex(idx, fill_value=0)\n",
    "    )\n",
    "\n",
    "    events_user = (\n",
    "    df_slice.groupby(\"userId\")\n",
    "    .size()\n",
    "    .reindex(idx, fill_value=0)\n",
    "    )\n",
    "    final_df[\"paid_perc\"] = 100 * premium_count / events_user.replace(0, 1)\n",
    "\n",
    "    # Produce final dataframe for output\n",
    "    final_df = final_df.join(session_stats.reindex(idx))\n",
    "    num_features = ['num_sessions', 'avg_songs_per_session', \\\n",
    "                    'avg_session_length', 'days_since_last_session']\n",
    "    final_df[num_features] = final_df[num_features].fillna(0).astype(float)\n",
    "    \n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = feature_builder(df_raw, pd.Timestamp('2018-10-20'))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4fef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_builder(df: pd.DataFrame,\n",
    "                 cutoff_date: pd.Timestamp,\n",
    "                 window_size: int = 10,\n",
    "                 buffer: int = 3) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Create churn labels for users based on cancellation events within a time window.\n",
    "    \n",
    "    Labels are created as follows:\n",
    "    - 1 (churned): User cancelled between cutoff_date and window_end\n",
    "    - 0 (stayed): User did not cancel or cancelled after buffer period\n",
    "    - NaN (excluded): User already churned before cutoff or churned during buffer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Raw event data containing user activity logs with columns:\n",
    "        time, userId, page (must include 'Cancellation Confirmation')\n",
    "    cutoff_date : pd.Timestamp\n",
    "        The observation cutoff date. Users active at this date are labeled.\n",
    "    window_size : int, default=10\n",
    "        Number of days after cutoff_date to observe for churn events.\n",
    "    buffer : int, default=3\n",
    "        Additional days after window_size where labels are marked as ambiguous (NaN).\n",
    "        Prevents labeling users who cancel shortly after window_end as negative.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Target labels indexed by userId with values:\n",
    "        1 = churned in window, 0 = stayed, NaN = ambiguous/already churned\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the time window including buffer period\n",
    "    window_end = cutoff_date + pd.Timedelta(days=window_size)\n",
    "    buffer_end = window_end + pd.Timedelta(days=buffer)\n",
    "    window_users = df.loc[df['time'] <= cutoff_date, 'userId'].unique()\n",
    "\n",
    "    # Get the cancellation time for each user\n",
    "    cancel_time = (\n",
    "        df.loc[df['page'] == 'Cancellation Confirmation']\n",
    "          .groupby('userId')['time']\n",
    "          .min()\n",
    "          .reindex(window_users)\n",
    "    )\n",
    "\n",
    "    # Set target labels based on cancellation time\n",
    "    y = pd.Series(0, index=window_users, name='target')\n",
    "    y[cancel_time <= cutoff_date] = np.nan\n",
    "    y[(cancel_time > window_end) & (cancel_time <= buffer_end)] = np.nan\n",
    "    y[(cancel_time > cutoff_date) & (cancel_time <= window_end)] = 1\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = label_builder(df_raw, pd.Timestamp('2018-10-20'))\n",
    "test_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_builder(df: pd.DataFrame,\n",
    "                start_date,\n",
    "                end_date,\n",
    "                *,\n",
    "                step_days: int = 10,\n",
    "                window_size: int = 10,\n",
    "                buffer: int = 3,\n",
    "                corr_threshold: float = 0.95,\n",
    "                categorical_cols=('level',),\n",
    "                verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Build multiple time-windowed feature-label datasets for temporal cross-validation.\n",
    "    \n",
    "    Creates sliding time windows where features are computed at each window date\n",
    "    and labels indicate churn in the subsequent period. Handles feature engineering,\n",
    "    label creation, correlation filtering, and data type conversions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Raw event data containing user activity logs.\n",
    "    start_date : str or pd.Timestamp\n",
    "        First cutoff date for window creation.\n",
    "    end_date : str or pd.Timestamp\n",
    "        Last cutoff date for window creation.\n",
    "    step_days : int, default=10\n",
    "        Number of days to advance between consecutive windows.\n",
    "    window_size : int, default=10\n",
    "        Number of days after each cutoff to observe for churn labels.\n",
    "    buffer : int, default=3\n",
    "        Buffer period days to exclude ambiguous labels near window edges.\n",
    "    corr_threshold : float or None, default=0.95\n",
    "        Correlation threshold for dropping redundant features. Set to None to skip.\n",
    "    categorical_cols : tuple, default=('level',)\n",
    "        Column names to convert to categorical dtype for LightGBM.\n",
    "    verbose : bool, default=True\n",
    "        Whether to print progress messages during window creation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix with all windows combined, categorical dtypes set,\n",
    "        and highly correlated features removed.\n",
    "    y : pd.Series\n",
    "        Target labels (0=stayed, 1=churned) corresponding to X rows.\n",
    "    groups : np.ndarray\n",
    "        User IDs for GroupKFold cross-validation to prevent data leakage.\n",
    "    df_window : pd.DataFrame\n",
    "        Complete dataset including features, target, and snapshot_date columns.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Uses GroupKFold-compatible groups to ensure the same user doesn't appear\n",
    "    in both training and validation sets during cross-validation.\n",
    "    \"\"\"\n",
    "\n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    end_date = pd.Timestamp(end_date)\n",
    "\n",
    "    all_windows = []\n",
    "    current = start_date\n",
    "\n",
    "    # Drop ambiguous users and build windows with features and labels\n",
    "    while current <= end_date:\n",
    "        if verbose:\n",
    "            print(f'  - Processing window: {current.date()}')\n",
    "\n",
    "        feats = feature_builder(df, current)\n",
    "        labels = label_builder(df, current, window_size=window_size, buffer=buffer)\n",
    "\n",
    "        labels = labels.reindex(feats.index)\n",
    "        mask = labels.notna()\n",
    "\n",
    "        window = feats.loc[mask].copy()\n",
    "        window['target'] = labels.loc[mask].astype(int)\n",
    "        window['snapshot_date'] = current\n",
    "\n",
    "        all_windows.append(window)\n",
    "        current += pd.Timedelta(days=step_days)\n",
    "\n",
    "    # Combines all windows into a single dataframe\n",
    "    df_window = pd.concat(all_windows, axis=0)\n",
    "\n",
    "    # Drop userId index\n",
    "    groups = df_window.index.to_numpy()\n",
    "    df_window = df_window.reset_index(drop=True)\n",
    "\n",
    "    # Define X and y\n",
    "    X = df_window.drop(columns=['target', 'snapshot_date'], errors='ignore')\n",
    "    y = df_window['target'].astype(int)\n",
    "\n",
    "    # Mark categoricals as category dtype\n",
    "    for c in categorical_cols:\n",
    "        if c in X.columns:\n",
    "            X[c] = X[c].astype('category')\n",
    "\n",
    "    # Drop highly correlated numeric columns\n",
    "    dropped_cols = []\n",
    "    if corr_threshold is not None:\n",
    "        X_num = X.select_dtypes(include=[np.number])\n",
    "        if X_num.shape[1] >= 2:\n",
    "            corr = X_num.corr().abs()\n",
    "            upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "            dropped_cols = [col for col in upper.columns if (upper[col] > corr_threshold).any()]\n",
    "            if verbose:\n",
    "                print(f'Dropping correlated (>{corr_threshold}): {dropped_cols}')\n",
    "            X = X.drop(columns=dropped_cols, errors='ignore')\n",
    "\n",
    "    return X, y, groups, df_window\n",
    "\n",
    "\n",
    "start_dt = pd.Timestamp('2018-10-01')\n",
    "end_dt = pd.Timestamp('2018-11-05')\n",
    "\n",
    "X_window, y_window, groups_window, df_window = window_builder(\n",
    "    df_raw,\n",
    "    start_dt,\n",
    "    end_dt\n",
    " )\n",
    "\n",
    "print(f'Total Samples: {len(df_window)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: feature_builder\n",
    "test_features = feature_builder(df_raw, pd.Timestamp('2018-10-20'))\n",
    "assert isinstance(test_features, pd.DataFrame), \"feature_builder should return DataFrame\"\n",
    "assert len(test_features) > 0, \"feature_builder should have rows\"\n",
    "assert 'level' in test_features.columns, \"Should have level column\"\n",
    "assert test_features['days_registered'].min() >= 0, \"Days registered should be non-negative\"\n",
    "print(\"✓ feature_builder tests passed\")\n",
    "\n",
    "# Test 2: label_builder\n",
    "test_labels = label_builder(df_raw, pd.Timestamp('2018-10-20'))\n",
    "assert isinstance(test_labels, pd.Series), \"label_builder should return Series\"\n",
    "assert test_labels.isin([0, 1]).sum() > 0, \"Should have 0/1 labels\"\n",
    "assert (test_labels.dropna() >= 0).all(), \"Labels should be non-negative\"\n",
    "print(\"✓ label_builder tests passed\")\n",
    "\n",
    "# Test 3: window_builder\n",
    "X_test_wb, y_test_wb, groups_test_wb, df_test_wb = window_builder(\n",
    "    df_raw, '2018-10-01', '2018-10-08', verbose=False\n",
    ")\n",
    "assert len(X_test_wb) == len(y_test_wb), \"X and y should have same length\"\n",
    "assert len(groups_test_wb) == len(y_test_wb), \"Groups should match y length\"\n",
    "assert y_test_wb.isin([0, 1]).all(), \"y should only contain 0/1\"\n",
    "print(\"✓ window_builder tests passed\")\n",
    "\n",
    "# Run Optuna optimization\n",
    "import optuna\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for LightGBM hyperparameter optimization.\n",
    "    \n",
    "    Samples hyperparameters from search space and evaluates model performance\n",
    "    using GroupKFold cross-validation with balanced accuracy as the metric.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trial : optuna.Trial\n",
    "        Optuna trial object for suggesting hyperparameters.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean balanced accuracy across all cross-validation folds.\n",
    "        Higher is better (optimization direction: maximize).\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Uses GroupKFold to prevent data leakage by ensuring the same user\n",
    "    never appears in both training and validation sets.\n",
    "    \"\"\"\n",
    "    # Sample hyperparameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'n_jobs': -1,\n",
    "        'is_unbalance': True,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Cross-validation with GroupKFold to prevent data leakage\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in gkf.split(X_window, y_window, groups=groups_window):\n",
    "        X_train, X_val = X_window.iloc[train_idx], X_window.iloc[val_idx]\n",
    "        y_train, y_val = y_window.iloc[train_idx], y_window.iloc[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_val)\n",
    "        score = balanced_accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Run optimization study\n",
    "study = optuna.create_study(direction='maximize', study_name='churn_lgb_balanced_acc')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'Optimization Complete!')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'Best Balanced Accuracy: {study.best_value:.4f}')\n",
    "print(f'\\nBest Hyperparameters:')\n",
    "for key, value in study.best_params.items():\n",
    "    print(f'  {key}: {value}')\n",
    "print(f'{\"=\"*60}\\n')\n",
    "\n",
    "# Train final ensemble with optimized params\n",
    "best_params = study.best_params.copy()\n",
    "best_params.update({'objective': 'binary', 'n_jobs': -1, 'is_unbalance': True, 'verbose': -1})\n",
    "\n",
    "optimized_models = []\n",
    "print(\"Training final ensemble with optimized hyperparameters...\")\n",
    "for i in range(5):\n",
    "    print(f'  - Training Optimized Model {i+1}/5...')\n",
    "    model = lgb.LGBMClassifier(**best_params, random_state=42+i)\n",
    "    model.fit(X_window, y_window)\n",
    "    optimized_models.append(model)\n",
    "\n",
    "print(\"\\nOptimized models ready for prediction!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec6613",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.62\n",
    "\n",
    "# Generate features for test set\n",
    "test_date = df_test['time'].max()\n",
    "X_test = feature_builder(df_test, test_date)\n",
    "\n",
    "# Match training dtype / categories for categoricals\n",
    "if 'level' in X_test.columns:\n",
    "    X_test['level'] = X_test['level'].astype('category')\n",
    "    if 'level' in X_window.columns and str(X_window['level'].dtype) == 'category':\n",
    "        X_test['level'] = X_test['level'].cat.set_categories(X_window['level'].cat.categories)\n",
    "\n",
    "model_test = optimized_models[0]\n",
    "trained_feature_names = list(model_test.booster_.feature_name())\n",
    "\n",
    "# Align columns to training features\n",
    "X_final = X_test.reindex(columns=trained_feature_names, fill_value=0)\n",
    "\n",
    "total_prob = np.zeros(len(X_final), dtype=float)\n",
    "\n",
    "# Aggregate predictions from optimized models\n",
    "for m in optimized_models:\n",
    "    prob = m.predict_proba(X_final)[:, 1]\n",
    "    total_prob += prob\n",
    "\n",
    "test_probs = total_prob / len(optimized_models)\n",
    "predict_labels = (test_probs >= threshold).astype(int)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({'id': X_test.index, 'target': predict_labels})\n",
    "submission.to_csv(root + '/data/submission_nopipeline.csv', index=False)\n",
    "\n",
    "print('Using threshold:', float(threshold))\n",
    "print('Predicted positives:', int(predict_labels.sum()), 'out of', int(len(predict_labels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
